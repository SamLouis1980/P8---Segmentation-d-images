{"cells":[{"cell_type":"markdown","metadata":{"id":"qMblz0Hg9Yjg"},"source":["# Projet 8 : Traitez les images pour le système embarqué d’une voiture autonome"]},{"cell_type":"markdown","metadata":{"id":"NtBZP6g49qHR"},"source":["* [1. Mission](#partie1)\n","    * [1.1 Contexte](#partie1.1)\n","    * [1.2 Objectifs](#partie1.2)\n","    * [1.3 Enjeux](#partie1.3)\n","* [2. Préparation de l'environnement](#partie2)\n","    * [2.1 Installation des modules](#partie2.1)\n","    * [2.2 MLFlow](#partie2.2)\n","    * [2.3 Librairies](#partie2.3)\n","    * [2.4 Fonctions](#partie2.4)\n","* [3. Exploration des données (EDA)](#partie3)\n","    * [3.1 Analyse de la Structure des Dossiers et des Fichiers](#partie3.1)\n","    * [3.2 Visualisation des Images et des Masques](#partie3.2)\n","    * [3.3 Distribution des Classes dans les Masques](#partie3.3)\n","    * [3.4 Analyse des Dimensions des Images et des Masques](#partie3.4)\n","    * [3.5 Analyse des Formats des Fichiers](#partie3.5)\n","    * [3.6 Compléter le Filtrage des Groupes](#partie3.6)\n","    * [3.7 Vérification de la Qualité des Données](#partie3.7)\n","    * [3.8 Vérification de la Correspondance des Fichiers d'Annotations](#partie3.8)\n","* [4. Prétraitement des données](#partie4)\n","    * [4.1 Data augmentation](#partie4.1)\n","    * [4.2 Data generator](#partie4.2)\n","    * [4.3 Charger le pipeline d'augmentation et initialiser les générateurs](#partie4.3)\n","* [5. Modélisation](#partie5)\n","    * [5.1 Modèle U-Net Mini](#partie5.1)\n","    * [5.2 Modèle U-Net avec ResNet34](#partie5.2)\n","    * [5.3 Modèle U-Net avec EfficientNetB0](#partie5.3)\n","    * [5.4 Modèle SegNet avec VGG16](#partie5.4)\n","    * [5.5 Modèle DeepLabV3+ avec ResNet50](#partie5.5)\n","    * [5.6 Modèle PSPnet avec ResNet50](#partie5.6)\n","    * [5.7 Modèle FPN avec ResNet50](#partie5.7)\n","* [6. Conclusion](#partie6)"]},{"cell_type":"markdown","metadata":{"id":"MgRNYdMq-LpH"},"source":["## <font color='red'>1. Mission</font><a class=\"anchor\" id=\"partie1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"eHNr5nxo-OtI"},"source":["##### <font color='blue'>1.1 Contexte</font><a class=\"anchor\" id=\"partie1.1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"4H5sunJJ-9wv"},"source":["Future Vision Transport est une entreprise innovante spécialisée dans la conception de systèmes embarqués de vision par ordinateur pour les véhicules autonomes. Ces systèmes visent à permettre aux véhicules d’analyser leur environnement en temps réel pour prendre des décisions précises et sécurisées.\n","\n","En tant qu’ingénieur en intelligence artificielle au sein de l’équipe R&D, ma mission porte spécifiquement sur la segmentation d’images, une étape clé dans la chaîne du système embarqué.\n","\n","Ce système se compose des quatre modules suivants :\n","- Acquisition des images en temps réel, qui capture les données visuelles.\n","- Traitement des images, qui prépare les images pour les étapes suivantes.\n","- Segmentation des images, votre domaine d’intervention.\n","- Système de décision, qui exploite les résultats de la segmentation pour guider le véhicule.\n","\n","Notre travail s’intègre entre le module de traitement des images (2), géré par Franck, et le système de décision (4), dirigé par Laura. La collaboration avec ces deux parties prenantes a permis de définir les contraintes et attentes suivantes :\n","\n","Franck, en charge du traitement des images, utilise le dataset Cityscapes, qui contient des annotations pour 32 sous-catégories. Cependant, seules les 8 catégories principales sont nécessaires pour notre tâche de segmentation.\n","Laura, en charge du système de décision, souhaite une API simple et efficace, capable de recevoir une image en entrée et de renvoyer un masque de segmentation en sortie."]},{"cell_type":"markdown","metadata":{"id":"7IqIe-5G-9li"},"source":["##### <font color='blue'>1.2 Objectifs</font><a class=\"anchor\" id=\"partie1.2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"IjXfmBgn_CHS"},"source":["Notre mission consiste à développer un système performant et exploitable de segmentation d’images qui respecte les contraintes techniques et fonctionnelles suivantes :\n","\n","- **Entraînement d’un modèle de segmentation sémantique** sur les 8 catégories principales du dataset Cityscapes, en utilisant Keras comme framework commun à toute l’équipe.\n","- **Création d’une API de prédiction** (via Flask ou FastAPI) pour permettre au système de décision de traiter facilement les résultats de la segmentation. Cette API devra être déployée sur le Cloud (Azure, Heroku, ou toute autre solution).\n","- **Conception d’une application web interactive** (via Flask ou Streamlit) pour tester l’API et visualiser les résultats des segmentations, en la rendant également accessible sur le Cloud."]},{"cell_type":"markdown","metadata":{"id":"QlagHfGQ_B4T"},"source":["##### <font color='blue'>1.3 Enjeux</font><a class=\"anchor\" id=\"partie1.3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"YpmZkLJ0_ud8"},"source":["La segmentation d’images est essentielle pour les systèmes de conduite autonome, car elle permet de détecter et de catégoriser les éléments critiques de l’environnement, comme les routes, les véhicules et les piétons. Le modèle développé devra être à la fois performant et optimisé pour s’intégrer dans une chaîne de traitement en temps réel, tout en restant léger et rapide à déployer.\n","\n","Ce projet représente une étape fondamentale dans le développement d’un système embarqué robuste et fiable pour les véhicules autonomes."]},{"cell_type":"markdown","metadata":{"id":"LoPkAon9Qghc"},"source":["## <font color='red'>2. Préparation de l'environnement</font><a class=\"anchor\" id=\"partie2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"7GLgVWl_7v22"},"source":["##### <font color='blue'>2.1 Installation des modules</font><a class=\"anchor\" id=\"partie2.1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9156,"status":"ok","timestamp":1737050906565,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"CSCkKs7l9RFe","outputId":"2fceac3d-703b-412f-a286-d0db4b4a6e2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mlflow\n","  Downloading mlflow-2.19.0-py3-none-any.whl.metadata (30 kB)\n","Collecting mlflow-skinny==2.19.0 (from mlflow)\n","  Downloading mlflow_skinny-2.19.0-py3-none-any.whl.metadata (31 kB)\n","Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n","Collecting alembic!=1.10.0,<2 (from mlflow)\n","  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting docker<8,>=4.0.0 (from mlflow)\n","  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n","Collecting graphene<4 (from mlflow)\n","  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n","Collecting gunicorn<24 (from mlflow)\n","  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n","Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n","Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n","Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n","Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (17.0.0)\n","Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.0)\n","Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n","Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.37)\n","Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.8)\n","Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n","Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.19.0->mlflow)\n","  Downloading databricks_sdk-0.40.0-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.44)\n","Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\n","Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n","Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n","Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (24.2)\n","Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (4.25.5)\n","Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n","Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n","  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n","Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n","  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n","Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n","  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2024.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n","Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.27.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.12)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n","Downloading mlflow-2.19.0-py3-none-any.whl (27.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading databricks_sdk-0.40.0-py3-none-any.whl (629 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m629.7/629.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n","Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, gunicorn, graphql-core, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n","Successfully installed Mako-1.3.8 alembic-1.14.0 databricks-sdk-0.40.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.19.0 mlflow-skinny-2.19.0\n"]}],"source":["!pip install mlflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2340,"status":"ok","timestamp":1737050908903,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"iBC6rB6D9A49","outputId":"d6e38e95-2c27-4055-bd72-e7ddea277c9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.3\n"]}],"source":["!pip install pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2747,"status":"ok","timestamp":1737051453458,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"MZjRBcfCC6Uk","outputId":"5132602b-6894-458b-e686-c82b1d606021"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting albumentations==1.3.0\n","  Downloading albumentations-1.3.0-py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.13.1)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (0.24.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (6.0.2)\n","Collecting qudida>=0.0.4 (from albumentations==1.3.0)\n","  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (4.10.0.84)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (1.6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.12.2)\n","Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (3.4.2)\n","Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (11.1.0)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2.36.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2024.12.12)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (24.2)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (3.5.0)\n","Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n","Installing collected packages: qudida, albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 2.0.0\n","    Uninstalling albumentations-2.0.0:\n","      Successfully uninstalled albumentations-2.0.0\n","Successfully installed albumentations-1.3.0 qudida-0.0.4\n"]}],"source":["!pip install albumentations==1.3.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":62400,"status":"ok","timestamp":1737051068943,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"f-juh1VcOWvd","outputId":"596ae056-d0e3-4ac9-e980-7174d9f568eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.12\n","  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting keras==2.12\n","  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting segmentation-models==1.0.1\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl.metadata (938 bytes)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.12.23)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n","  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.69.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.12.1)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.33)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n","Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n","  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.25.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n","Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n","  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n","Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n","  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.12.2)\n","Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n","  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n","Collecting keras-applications<=1.0.8,>=1.0.7 (from segmentation-models==1.0.1)\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n","Collecting image-classifiers==1.0.0 (from segmentation-models==1.0.1)\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl.metadata (8.6 kB)\n","Collecting efficientnet==1.0.0 (from segmentation-models==1.0.1)\n","  Downloading efficientnet-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from efficientnet==1.0.0->segmentation-models==1.0.1) (0.25.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n","Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.33)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n","INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n","Collecting jax>=0.3.15 (from tensorflow==2.12)\n","  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12)\n","  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Collecting jax>=0.3.15 (from tensorflow==2.12)\n","  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n","  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Collecting jax>=0.3.15 (from tensorflow==2.12)\n","  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n","  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n","  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n","Collecting jax>=0.3.15 (from tensorflow==2.12)\n","  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n","  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n","Collecting jax>=0.3.15 (from tensorflow==2.12)\n","  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n","  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n","Collecting jax>=0.3.15 (from tensorflow==2.12)\n","  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n","  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.13.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.27.0)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n","INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n","Collecting scikit-image (from efficientnet==1.0.0->segmentation-models==1.0.1)\n","  Downloading scikit_image-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.4.2)\n","Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (11.1.0)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.36.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2024.12.12)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (0.4)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n","Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_image-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, numpy, keras, gast, scikit-image, keras-applications, jaxlib, google-auth-oauthlib, tensorboard, jax, image-classifiers, efficientnet, tensorflow, segmentation-models\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.17.0\n","    Uninstalling wrapt-1.17.0:\n","      Successfully uninstalled wrapt-1.17.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.5.0\n","    Uninstalling keras-3.5.0:\n","      Successfully uninstalled keras-3.5.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.6.0\n","    Uninstalling gast-0.6.0:\n","      Successfully uninstalled gast-0.6.0\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.25.0\n","    Uninstalling scikit-image-0.25.0:\n","      Successfully uninstalled scikit-image-0.25.0\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.4.33\n","    Uninstalling jaxlib-0.4.33:\n","      Successfully uninstalled jaxlib-0.4.33\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.1\n","    Uninstalling google-auth-oauthlib-1.2.1:\n","      Successfully uninstalled google-auth-oauthlib-1.2.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.1\n","    Uninstalling tensorboard-2.17.1:\n","      Successfully uninstalled tensorboard-2.17.1\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.4.33\n","    Uninstalling jax-0.4.33:\n","      Successfully uninstalled jax-0.4.33\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.1\n","    Uninstalling tensorflow-2.17.1:\n","      Successfully uninstalled tensorflow-2.17.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 2.0.0 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.1.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","bigframes 1.31.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\n","chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","blosc2 3.0.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed efficientnet-1.0.0 gast-0.4.0 google-auth-oauthlib-1.0.0 image-classifiers-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 keras-applications-1.0.8 numpy-1.23.5 scikit-image-0.24.0 segmentation-models-1.0.1 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"c828ec8a5a66492ba3caf5d8f0148b1d"}},"metadata":{}}],"source":["!pip install tensorflow==2.12 keras==2.12 segmentation-models==1.0.1"]},{"cell_type":"code","source":["!pip install numpy==1.23.5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475},"id":"5fXkuFAwUSd_","executionInfo":{"status":"ok","timestamp":1737051419622,"user_tz":-60,"elapsed":5139,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"}},"outputId":"ad8ee555-4a11-4047-cfae-c92f99870860"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.23.5\n","  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.24.3\n","    Uninstalling numpy-1.24.3:\n","      Successfully uninstalled numpy-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 2.0.0 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.1.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","bigframes 1.31.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\n","chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","blosc2 3.0.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"c2cc262d74a74830b88634891fae41dd"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"AksvG5pv-XSF"},"source":["##### <font color='blue'>2.2 MLFlow</font><a class=\"anchor\" id=\"partie2.2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2167,"status":"ok","timestamp":1737051192214,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"1_6K8y26-bG5","outputId":"70e33949-e74a-4070-da51-203ef3fbeb04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","MLFlow Tracking UI: NgrokTunnel: \"https://8d8a-34-118-240-185.ngrok-free.app\" -> \"http://localhost:5000\"\n"]}],"source":["import subprocess\n","from pyngrok import ngrok\n","\n","# Démarrer MLFlow en arrière-plan\n","mlflow_server = subprocess.Popen([\"mlflow\", \"ui\", \"--port\", \"5000\"])\n","\n","# Ajouter mon authtoken Ngrok (remplace par ton propre token)\n","!ngrok config add-authtoken 2o7fRSSTsKkRHY1jl4VoX9qS7AR_5TCYPvixQ8rv7g5PqJp8t\n","\n","# Créer un tunnel pour accéder à MLFlow UI\n","public_url = ngrok.connect(5000)\n","print(\"MLFlow Tracking UI:\", public_url)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2690,"status":"ok","timestamp":1737051194902,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"_-F50Mbv-no0","outputId":"a4691489-53ca-442f-9f99-29ba6661d2a7"},"outputs":[{"output_type":"stream","name":"stderr","text":["2025/01/16 18:13:13 INFO mlflow.tracking.fluent: Experiment with name 'P8 - Image Segmentation' does not exist. Creating a new experiment.\n"]},{"output_type":"execute_result","data":{"text/plain":["<Experiment: artifact_location='file:///content/mlruns/203819702506780593', creation_time=1737051193937, experiment_id='203819702506780593', last_update_time=1737051193937, lifecycle_stage='active', name='P8 - Image Segmentation', tags={}>"]},"metadata":{},"execution_count":3}],"source":["import mlflow\n","mlflow.set_experiment(\"P8 - Image Segmentation\")"]},{"cell_type":"markdown","metadata":{"id":"-RPn4q3I75t9"},"source":["##### <font color='blue'>2.3 Librairies</font><a class=\"anchor\" id=\"partie2.3\"></a>"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IKwXfLKcS4Uw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","from PIL import Image\n","import segmentation_models as sm\n","import albumentations as A\n","import tensorflow as tf\n","import time\n","import datetime\n","from tensorflow.keras import layers, Model\n","from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, Activation, MaxPooling2D, UpSampling2D, Concatenate, GlobalAveragePooling2D, Reshape, Lambda, Dropout\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, jaccard_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.metrics import MeanIoU\n","import cv2\n","from segmentation_models.metrics import IOUScore, FScore\n","from segmentation_models.losses import DiceLoss\n","import json"],"metadata":{"id":"EOhqP_IDNcqu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2vUP3wI79wq"},"source":["##### <font color='blue'>2.4 Fonctions</font><a class=\"anchor\" id=\"partie2.4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGVx_zE58Eps"},"outputs":[],"source":["def load_file_paths(images_path, masks_path, data_cat=\"train\"):\n","    \"\"\"\n","    Charge les chemins des images et des masques pour un type de données spécifique.\n","\n","    Args:\n","        images_path (str): Chemin du dossier des images.\n","        masks_path (str): Chemin du dossier des masques.\n","        data_cat (str): Type de données (\"train\", \"val\", \"test\").\n","\n","    Returns:\n","        list, list: Listes des chemins des images et des masques.\n","    \"\"\"\n","    images_dir = os.path.join(images_path, data_cat)\n","    masks_dir = os.path.join(masks_path, data_cat)\n","\n","    # Vérification des répertoires\n","    if not os.path.exists(images_dir):\n","        raise FileNotFoundError(f\"Le répertoire {images_dir} n'existe pas.\")\n","    if not os.path.exists(masks_dir):\n","        raise FileNotFoundError(f\"Le répertoire {masks_dir} n'existe pas.\")\n","\n","    cities = os.listdir(images_dir)\n","    image_files, mask_files = [], []\n","\n","    for city in cities:\n","        city_img_dir = os.path.join(images_dir, city)\n","        city_mask_dir = os.path.join(masks_dir, city)\n","\n","        # Vérifier si le sous-dossier existe dans \"masks_path\"\n","        if not os.path.exists(city_mask_dir):\n","            print(f\"Attention : Le dossier des masques pour la ville '{city}' est manquant.\")\n","            continue\n","\n","        # Charger les fichiers\n","        city_images = [os.path.join(city_img_dir, f) for f in os.listdir(city_img_dir) if f.endswith(\"_leftImg8bit.png\")]\n","        city_masks = [os.path.join(city_mask_dir, f) for f in os.listdir(city_mask_dir) if f.endswith(\"_gtFine_labelIds.png\")]\n","\n","        if len(city_images) != len(city_masks):\n","            print(f\"Attention : Incohérence dans le nombre de fichiers pour la ville '{city}'.\")\n","\n","        image_files.extend(sorted(city_images))\n","        mask_files.extend(sorted(city_masks))\n","\n","    return image_files, mask_files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRiLEcdPBre4"},"outputs":[],"source":["def show_image_and_mask(image_path, mask_path, save_path=None):\n","    \"\"\"\n","    Affiche une image et son masque côte à côte.\n","    Si un chemin est fourni, enregistre la figure au format PNG.\n","\n","    Args:\n","        image_path (str): Chemin de l'image RGB.\n","        mask_path (str): Chemin du masque associé.\n","        save_path (str, optional): Chemin pour sauvegarder l'image.\n","    \"\"\"\n","    img = Image.open(image_path)\n","    mask = Image.open(mask_path)\n","\n","    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","\n","    ax[0].imshow(img)\n","    ax[0].set_title(\"Image RGB\")\n","    ax[0].axis(\"off\")\n","\n","    ax[1].imshow(mask)\n","    ax[1].set_title(\"Masque (color)\")\n","    ax[1].axis(\"off\")\n","\n","    # Enregistrer l’image si un chemin est fourni\n","    if save_path:\n","        fig.savefig(save_path, bbox_inches=\"tight\")\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr_uWrwWD8zH"},"outputs":[],"source":["def analyze_class_distribution(masks_path):\n","    \"\"\"\n","    Analyse la distribution des classes dans les masques d'annotations.\n","\n","    Args:\n","        masks_path (str): Chemin vers le dossier contenant les masques.\n","\n","    Returns:\n","        dict: Dictionnaire contenant la distribution des classes.\n","    \"\"\"\n","    class_counts = Counter()\n","\n","    for root, _, files in os.walk(masks_path):\n","        for file in files:\n","            if file.endswith(\"_gtFine_labelIds.png\"):\n","                mask_path = os.path.join(root, file)\n","                mask = np.array(Image.open(mask_path))\n","                class_counts.update(mask.flatten())\n","\n","    return class_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8gtvHLaEeAM"},"outputs":[],"source":["def plot_class_distribution(class_counts, save_path=None):\n","    \"\"\"\n","    Affiche et enregistre la distribution des classes sous forme de graphique.\n","\n","    Args:\n","        class_counts (dict): Distribution des classes sous forme de dictionnaire.\n","        save_path (str, optional): Chemin pour sauvegarder le graphique. Si None, il affiche seulement.\n","    \"\"\"\n","    plt.figure(figsize=(10, 5))\n","    plt.bar(class_counts.keys(), class_counts.values(), color=\"skyblue\")\n","    plt.xlabel(\"Classes\")\n","    plt.ylabel(\"Nombre de pixels\")\n","    plt.title(\"Distribution des classes\")\n","\n","    if save_path:\n","        plt.savefig(save_path)\n","        plt.close()\n","    else:\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9r506w-Z37B"},"outputs":[],"source":["def analyze_image_dimensions(images_path, masks_path):\n","    \"\"\"\n","    Analyse les dimensions des images et des masques.\n","\n","    Args:\n","        images_path (str): Chemin du dossier contenant les images.\n","        masks_path (str): Chemin du dossier contenant les masques.\n","\n","    Returns:\n","        dict: Dictionnaire contenant les dimensions des images et des masques.\n","    \"\"\"\n","    dimensions = {\"images\": [], \"masks\": []}\n","\n","    for root, _, files in os.walk(images_path):\n","        for file in files:\n","            if file.endswith(\"_leftImg8bit.png\"):\n","                img = Image.open(os.path.join(root, file))\n","                dimensions[\"images\"].append(img.size)\n","\n","    for root, _, files in os.walk(masks_path):\n","        for file in files:\n","            if file.endswith(\"_gtFine_labelIds.png\"):\n","                mask = Image.open(os.path.join(root, file))\n","                dimensions[\"masks\"].append(mask.size)\n","\n","    # Compter la fréquence des dimensions\n","    img_dim_counts = Counter(dimensions[\"images\"])\n","    mask_dim_counts = Counter(dimensions[\"masks\"])\n","\n","    return img_dim_counts, mask_dim_counts\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOM-dma4iUiS"},"outputs":[],"source":["def analyze_file_formats(images_path, masks_path):\n","    image_formats = set()\n","    mask_formats = set()\n","\n","    for root, _, files in os.walk(images_path):\n","        for file in files:\n","            image_formats.add(os.path.splitext(file)[1])\n","\n","    for root, _, files in os.walk(masks_path):\n","        for file in files:\n","            mask_formats.add(os.path.splitext(file)[1])\n","\n","    print(\"Formats des fichiers d'images :\", image_formats)\n","    print(\"Formats des fichiers de masques :\", mask_formats)\n","\n","    return image_formats, mask_formats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhBXC7r0rjEk"},"outputs":[],"source":["def filter_groups_in_mask(mask_path, class_to_group):\n","    mask = np.array(Image.open(mask_path))\n","    group_mask = np.zeros_like(mask)\n","    for cls, grp in class_to_group.items():\n","        group_mask[mask == cls] = grp\n","    return group_mask\n","\n","def apply_cityscapes_palette(group_mask):\n","    cityscapes_palette = [\n","        (128, 64, 128),  # road (flat)\n","        (244, 35, 232),  # sidewalk (flat)\n","        (70, 70, 70),    # building (construction)\n","        (102, 102, 156), # wall (construction)\n","        (190, 153, 153), # fence (construction)\n","        (153, 153, 153), # pole (object)\n","        (250, 170, 30),  # traffic light (object)\n","        (220, 220, 0),   # traffic sign (object)\n","        (107, 142, 35),  # vegetation (nature)\n","        (152, 251, 152), # terrain (nature)\n","        (70, 130, 180),  # sky (sky)\n","        (220, 20, 60),   # person (human)\n","        (255, 0, 0),     # rider (human)\n","        (0, 0, 142),     # car (vehicle)\n","        (0, 0, 70),      # truck (vehicle)\n","        (0, 60, 100),    # bus (vehicle)\n","        (0, 80, 100),    # on rails (vehicle)\n","        (0, 0, 230),     # motorcycle (vehicle)\n","        (119, 11, 32),   # bicycle (vehicle)\n","        (0, 0, 0)        # void\n","    ] + [(0, 0, 0)] * (256 - 20)\n","\n","    pil_mask = Image.fromarray(group_mask.astype('uint8'))\n","    flat_palette = [value for color in cityscapes_palette for value in color]\n","    pil_mask.putpalette(flat_palette)\n","    return pil_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGqNdlLnsepf"},"outputs":[],"source":["def check_file_integrity(images_path, masks_path):\n","    issues = {\"unreadable_images\": [], \"unreadable_masks\": []}\n","\n","    # Vérifier les images\n","    for root, _, files in os.walk(images_path):\n","        for file in files:\n","            if file.endswith(\".png\"):\n","                file_path = os.path.join(root, file)\n","                try:\n","                    img = Image.open(file_path)\n","                    img.verify()\n","                except Exception as e:\n","                    issues[\"unreadable_images\"].append((file_path, str(e)))\n","                    os.remove(file_path)  # Suppression automatique\n","\n","    # Vérifier les masques\n","    for root, _, files in os.walk(masks_path):\n","        for file in files:\n","            if file.endswith(\".png\"):\n","                file_path = os.path.join(root, file)\n","                try:\n","                    mask = Image.open(file_path)\n","                    mask.verify()\n","                except Exception as e:\n","                    issues[\"unreadable_masks\"].append((file_path, str(e)))\n","                    os.remove(file_path)  # Suppression automatique\n","\n","    print(f\"Nombre d'images illisibles supprimées : {len(issues['unreadable_images'])}\")\n","    print(f\"Nombre de masques illisibles supprimés : {len(issues['unreadable_masks'])}\")\n","    return issues"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkNfAFT8x2QN"},"outputs":[],"source":["def check_annotation_completeness(images_path, masks_path):\n","    \"\"\"\n","    Vérifie que chaque image RGB a ses fichiers d'annotations associés.\n","\n","    Args:\n","        images_path (str): Chemin vers le dossier contenant les images.\n","        masks_path (str): Chemin vers le dossier contenant les masques.\n","\n","    Returns:\n","        list: Liste des problèmes de correspondance (image, fichier manquant).\n","    \"\"\"\n","    issues = []\n","    for root, _, files in os.walk(images_path):\n","        for file in files:\n","            if file.endswith(\"_leftImg8bit.png\"):\n","                base_name = file.replace(\"_leftImg8bit.png\", \"\")\n","                mask_dir = root.replace(\"leftImg8bit\", \"gtFine\")\n","                expected_files = [\n","                    f\"{base_name}_gtFine_color.png\",\n","                    f\"{base_name}_gtFine_instanceIds.png\",\n","                    f\"{base_name}_gtFine_labelIds.png\",\n","                    f\"{base_name}_gtFine_polygons.json\",\n","                ]\n","                # Vérifier la présence des fichiers\n","                for expected_file in expected_files:\n","                    if not os.path.exists(os.path.join(mask_dir, expected_file)):\n","                        issues.append((file, expected_file))\n","    return issues"]},{"cell_type":"markdown","metadata":{"id":"Jr01aPZPQwmP"},"source":["## <font color='red'>3. Exploration des données (EDA)</font><a class=\"anchor\" id=\"partie3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"mgEl-OZmQ0wj"},"source":["##### <font color='blue'>3.1 Analyse de la Structure des Dossiers et des Fichiers</font><a class=\"anchor\" id=\"partie3.1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1u6pGrxRQ3ww"},"outputs":[],"source":["# **Démarrer un run MLFlow**\n","with mlflow.start_run(run_name=\"EDA_Analyse_Structure\"):\n","\n","    # Définir les chemins vers les images et les masques\n","    images_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_leftImg8bit_trainvaltest/leftImg8bit\"\n","    masks_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine\"\n","\n","    # Vérification des dossiers\n","    images_dirs = os.listdir(images_path)\n","    masks_dirs = os.listdir(masks_path)\n","\n","    print(\"Contenu des dossiers :\")\n","    print(\"Images :\", images_dirs)\n","    print(\"Masques :\", masks_dirs)\n","\n","    # **Enregistrement des informations dans MLFlow**\n","    mlflow.log_param(\"Nombre de dossiers images\", len(images_dirs))\n","    mlflow.log_param(\"Nombre de dossiers masques\", len(masks_dirs))\n","\n","    # **Chargement des chemins des fichiers (sans les intégrer à MLFlow maintenant)**\n","    train_images, train_masks = load_file_paths(images_path, masks_path, \"train\")\n","    val_images, val_masks = load_file_paths(images_path, masks_path, \"val\")\n","    test_images, test_masks = load_file_paths(images_path, masks_path, \"test\")\n","\n","    # **Enregistrement des métriques dans MLFlow**\n","    mlflow.log_metric(\"nb_images_train\", len(train_images))\n","    mlflow.log_metric(\"nb_masks_train\", len(train_masks))\n","    mlflow.log_metric(\"nb_images_val\", len(val_images))\n","    mlflow.log_metric(\"nb_masks_val\", len(val_masks))\n","    mlflow.log_metric(\"nb_images_test\", len(test_images))\n","    mlflow.log_metric(\"nb_masks_test\", len(test_masks))\n","\n","    print(\"Analyse de la structure des fichiers terminée et enregistrée dans MLFlow.\")"]},{"cell_type":"markdown","metadata":{"id":"LXWSd81EQ7n9"},"source":["##### <font color='blue'>3.2 Visualisation des Images et des Masques</font><a class=\"anchor\" id=\"partie3.2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tn-T1oINQ-OS"},"outputs":[],"source":["import tempfile\n","\n","with mlflow.start_run(run_name=\"EDA_Visualisation\"):\n","    image_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_leftImg8bit_trainvaltest/leftImg8bit/train/aachen/aachen_000000_000019_leftImg8bit.png\"\n","    mask_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine/train/aachen/aachen_000000_000019_gtFine_color.png\"\n","\n","    # Créer un fichier temporaire pour sauvegarder l’image\n","    temp_img_path = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False).name\n","\n","    # Affichage et sauvegarde en même temps\n","    show_image_and_mask(image_path, mask_path, save_path=temp_img_path)\n","\n","    # Enregistrement dans MLFlow\n","    mlflow.log_artifact(temp_img_path, artifact_path=\"EDA_Images\")\n","\n","    print(f\"Image enregistrée dans MLFlow : {temp_img_path}\")"]},{"cell_type":"markdown","metadata":{"id":"xKlMacoXRBCb"},"source":["##### <font color='blue'>3.3 Distribution des Classes dans les Masques</font><a class=\"anchor\" id=\"partie3.3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvcMs8_WREWQ"},"outputs":[],"source":["with mlflow.start_run(run_name=\"EDA_Distribution_Classes\"):\n","    masks_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine/train\"\n","\n","    # Exécuter l'analyse\n","    class_counts = analyze_class_distribution(masks_path)\n","\n","    # Loguer les métriques des classes\n","    for cls, count in class_counts.items():\n","        mlflow.log_metric(f\"class_{cls}_pixels\", count)\n","\n","    # Sauvegarde du graphique dans MLFlow\n","    temp_plot_path = \"/tmp/class_distribution.png\"\n","    plot_class_distribution(class_counts, save_path=temp_plot_path)\n","    mlflow.log_artifact(temp_plot_path, artifact_path=\"EDA_Images\")"]},{"cell_type":"markdown","metadata":{"id":"Pn_ODNJaRMa_"},"source":["##### <font color='blue'>3.4 Analyse des Dimensions des Images et des Masques</font><a class=\"anchor\" id=\"partie3.4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIpJX5JyRPC9"},"outputs":[],"source":["with mlflow.start_run(run_name=\"EDA_Analyse_Dimensions\"):\n","    images_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_leftImg8bit_trainvaltest/leftImg8bit\"\n","    masks_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine\"\n","\n","    # Exécuter l’analyse des dimensions\n","    img_dim_counts, mask_dim_counts = analyze_image_dimensions(images_path, masks_path)\n","\n","    # Loguer les tailles uniques sous forme de paramètres\n","    mlflow.log_param(\"nb_dimensions_images_uniques\", len(img_dim_counts))\n","    mlflow.log_param(\"nb_dimensions_masques_uniques\", len(mask_dim_counts))\n","\n","    # Loguer chaque dimension et sa fréquence\n","    for dim, count in img_dim_counts.items():\n","        dim_str = f\"dim_img_{dim[0]}x{dim[1]}\"\n","        mlflow.log_metric(dim_str, count)\n","\n","    for dim, count in mask_dim_counts.items():\n","        dim_str = f\"dim_mask_{dim[0]}x{dim[1]}\"\n","        mlflow.log_metric(dim_str, count)\n"]},{"cell_type":"markdown","metadata":{"id":"Rf2nqJWPRSDt"},"source":["##### <font color='blue'>3.5 Analyse des Formats des Fichiers</font><a class=\"anchor\" id=\"partie3.5\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S648YL6ORVNS"},"outputs":[],"source":["with mlflow.start_run(run_name=\"EDA_Analyse_Formats\"):\n","    images_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_leftImg8bit_trainvaltest/leftImg8bit\"\n","    masks_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine\"\n","\n","    # Exécuter l’analyse des formats\n","    img_formats, mask_formats = analyze_file_formats(images_path, masks_path)\n","\n","    # Loguer les formats uniques sous forme de paramètres\n","    mlflow.log_param(\"formats_images_uniques\", \", \".join(sorted(img_formats)))\n","    mlflow.log_param(\"formats_masques_uniques\", \", \".join(sorted(mask_formats)))\n","\n","    # Loguer le nombre de fichiers par format sous forme de métriques\n","    for img_format in img_formats:\n","        img_format_clean = img_format.replace(\".\", \"\").lower()\n","        mlflow.log_metric(f\"nb_fichiers_images_{img_format_clean}\",\n","                          sum(1 for root, _, files in os.walk(images_path) if any(f.endswith(img_format) for f in files)))\n","\n","    for mask_format in mask_formats:\n","        mask_format_clean = mask_format.replace(\".\", \"\").lower()\n","        mlflow.log_metric(f\"nb_fichiers_masques_{mask_format_clean}\",\n","                          sum(1 for root, _, files in os.walk(masks_path) if any(f.endswith(mask_format) for f in files)))\n"]},{"cell_type":"markdown","metadata":{"id":"RnwXQyNPRX6U"},"source":["##### <font color='blue'>3.6 Compléter le Filtrage des Groupes</font><a class=\"anchor\" id=\"partie3.6\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHb1XLA4RadG"},"outputs":[],"source":["with mlflow.start_run(run_name=\"EDA_Filtrage_Groupes\"):\n","    # Définition des classes et groupes\n","    class_to_group = {\n","        -1: 0, 0: 0, 1: 0, 2: 0,  # void\n","        7: 1, 8: 1, 9: 1, 10: 1,  # flat\n","        11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 2,  # construction\n","        17: 3, 18: 3, 19: 3, 20: 3,  # object\n","        21: 4, 22: 4,  # nature\n","        23: 5,  # sky\n","        24: 6, 25: 6,  # human\n","        26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7, 32: 7, 33: 7  # vehicle\n","    }\n","\n","    example_mask_path = \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine/train/aachen/aachen_000000_000019_gtFine_labelIds.png\"\n","\n","    # Application du filtrage des groupes\n","    filtered_group_mask = filter_groups_in_mask(example_mask_path, class_to_group)\n","    colored_mask = apply_cityscapes_palette(filtered_group_mask)\n","\n","    # Loguer le nombre de pixels par groupe\n","    unique, counts = np.unique(filtered_group_mask, return_counts=True)\n","    group_distribution = dict(zip(unique, counts))\n","\n","    for group, count in group_distribution.items():\n","        mlflow.log_metric(f\"group_{group}_pixels\", count)\n","\n","    # Sauvegarder l'image filtrée et colorisée dans MLFlow\n","    mask_output_path = \"filtered_mask.png\"\n","    colored_mask.save(mask_output_path)\n","    mlflow.log_artifact(mask_output_path)\n","\n","    print(\"Filtrage des groupes et enregistrement des résultats terminés dans MLFlow.\")"]},{"cell_type":"markdown","metadata":{"id":"rqgYfzDwRd4a"},"source":["##### <font color='blue'>3.7 Vérification de la Qualité des Données</font><a class=\"anchor\" id=\"partie3.7\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFJRByjjRuV8"},"outputs":[],"source":["import json\n","\n","with mlflow.start_run(run_name=\"EDA_Verification_Qualite\"):\n","    # Vérification de l'intégrité des fichiers\n","    issues = check_file_integrity(\n","        \"/content/drive/My Drive/projet 8/P8_Cityscapes_leftImg8bit_trainvaltest/leftImg8bit\",\n","        \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine\"\n","    )\n","\n","    # Loguer les métriques\n","    mlflow.log_metric(\"images_illisibles_supprimees\", len(issues[\"unreadable_images\"]))\n","    mlflow.log_metric(\"masques_illisibles_supprimes\", len(issues[\"unreadable_masks\"]))\n","\n","    # Sauvegarder la liste des fichiers supprimés dans MLFlow\n","    deleted_files_path = \"deleted_files.json\"\n","    with open(deleted_files_path, \"w\") as f:\n","        json.dump(issues, f, indent=4)\n","\n","    mlflow.log_artifact(deleted_files_path)\n","\n","    print(\"Vérification de la qualité des données terminée et enregistrée dans MLFlow.\")\n"]},{"cell_type":"markdown","metadata":{"id":"gOHkYkQxXRwD"},"source":["##### <font color='blue'>3.8 Vérification de la Correspondance des Fichiers d'Annotations</font><a class=\"anchor\" id=\"partie3.8\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhcxVlcMXU94"},"outputs":[],"source":["with mlflow.start_run(run_name=\"EDA_Verification_Annotations\"):\n","    # Vérification de la correspondance des fichiers d'annotations\n","    annotation_issues = check_annotation_completeness(\n","        \"/content/drive/My Drive/projet 8/P8_Cityscapes_leftImg8bit_trainvaltest/leftImg8bit\",\n","        \"/content/drive/My Drive/projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine\",\n","    )\n","\n","    # Loguer le nombre de fichiers manquants\n","    mlflow.log_metric(\"problèmes_annotations\", len(annotation_issues))\n","\n","    # Sauvegarder la liste des fichiers manquants dans MLFlow\n","    annotation_issues_path = \"annotation_issues.json\"\n","    with open(annotation_issues_path, \"w\") as f:\n","        json.dump(annotation_issues, f, indent=4)\n","\n","    mlflow.log_artifact(annotation_issues_path)\n","\n","    # Affichage des résultats dans le notebook\n","    if annotation_issues:\n","        print(f\"Nombre de problèmes détectés : {len(annotation_issues)}\")\n","        print(\"Exemples de problèmes :\")\n","        for img, missing in annotation_issues[:10]:\n","            print(f\"Image : {img}, Fichier manquant : {missing}\")\n","    else:\n","        print(\"Toutes les images ont leurs fichiers d'annotations associés.\")\n","\n","    print(\"Vérification des annotations terminée et enregistrée dans MLFlow.\")"]},{"cell_type":"markdown","metadata":{"id":"ANNYv9y3YyW4"},"source":["## <font color='red'>4. Prétraitement des données (EDA)</font><a class=\"anchor\" id=\"partie4\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ahX70CrmZImu"},"source":["##### <font color='blue'>4.1 Data augmentation</font><a class=\"anchor\" id=\"partie4.1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLAiOoNXCrKZ"},"outputs":[],"source":["def get_augmentation_pipeline():\n","    \"\"\"\n","    Crée une pipeline d'augmentations dynamiques pour les images et les masques.\n","    Returns:\n","        albumentations.Compose: Pipeline d'augmentations.\n","    \"\"\"\n","    augmentation = A.Compose([\n","        A.HorizontalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.2),\n","        A.Resize(height=256, width=256)\n","    ])\n","\n","    # Loguer la configuration de l'augmentation dans MLFlow\n","    mlflow.log_param(\"augmentation_pipeline\", str(augmentation))\n","\n","    return augmentation"]},{"cell_type":"markdown","metadata":{"id":"-qyyunQ7CtMN"},"source":["##### <font color='blue'>4.2 Data generator</font><a class=\"anchor\" id=\"partie4.2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZsfebfUCv_w"},"outputs":[],"source":["class DataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"\n","    Générateur de données avec augmentation dynamique pour l'entraînement et la validation.\n","    \"\"\"\n","\n","    def __init__(self, image_paths, mask_paths, batch_size, img_size=(256, 256), augmentation=None, shuffle=True, class_to_group=None, num_classes=8):\n","        \"\"\"\n","        Initialisation du générateur de données.\n","        \"\"\"\n","        assert len(image_paths) == len(mask_paths), \"Les listes d'images et de masques doivent avoir la même longueur.\"\n","        self.image_paths = image_paths\n","        self.mask_paths = mask_paths\n","        self.batch_size = batch_size\n","        self.img_size = img_size\n","        self.augmentation = augmentation\n","        self.shuffle = shuffle\n","        self.class_to_group = class_to_group\n","        self.num_classes = num_classes\n","        self.indexes = np.arange(len(self.image_paths))\n","        self.on_epoch_end()\n","\n","        # Loguer les paramètres du DataGenerator dans MLFlow\n","        mlflow.log_param(\"batch_size\", batch_size)\n","        mlflow.log_param(\"image_size\", img_size)\n","        mlflow.log_param(\"num_classes\", num_classes)\n","        mlflow.log_param(\"shuffle\", shuffle)\n","\n","    def __len__(self):\n","        \"\"\"Nombre de lots par époque.\"\"\"\n","        return int(np.floor(len(self.image_paths) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        \"\"\"Génère un lot de données.\"\"\"\n","        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n","        image_paths_temp = [self.image_paths[k] for k in indexes]\n","        mask_paths_temp = [self.mask_paths[k] for k in indexes]\n","        X, y = self.__data_generation(image_paths_temp, mask_paths_temp)\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        \"\"\"Mélange les données après chaque époque.\"\"\"\n","        self.indexes = np.arange(len(self.image_paths))\n","        if self.shuffle:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, image_paths_temp, mask_paths_temp):\n","        \"\"\"Prépare les lots.\"\"\"\n","        X = np.empty((self.batch_size, *self.img_size, 3), dtype=np.float32)\n","        y = np.empty((self.batch_size, *self.img_size, self.num_classes), dtype=np.float32)\n","\n","        i = 0\n","        while i < self.batch_size:\n","            idx = np.random.randint(len(image_paths_temp))\n","            image_path = image_paths_temp[idx]\n","            mask_path = mask_paths_temp[idx]\n","\n","            image = cv2.imread(image_path)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            image = cv2.resize(image, (self.img_size[1], self.img_size[0]))\n","\n","            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","            mask = cv2.resize(mask, (self.img_size[1], self.img_size[0]), interpolation=cv2.INTER_NEAREST)  # ✅ Correction\n","\n","            unique_classes = np.unique(mask)\n","            if len(unique_classes) < 3:\n","                continue\n","\n","            if self.augmentation:\n","                augmented = self.augmentation(image=image, mask=mask)\n","                image, mask = augmented['image'], augmented['mask']\n","\n","            X[i] = image / 255.0\n","            y[i] = self._remap_and_one_hot_encode(mask)\n","            i += 1\n","\n","        return X, y\n","\n","    def _remap_and_one_hot_encode(self, mask):\n","        \"\"\"Remappe les classes de masque et effectue un encodage one-hot.\"\"\"\n","        remapped_mask = np.zeros_like(mask, dtype=np.int32)\n","\n","        if self.class_to_group:\n","            for cls, grp in self.class_to_group.items():\n","                remapped_mask[mask == cls] = grp\n","        else:\n","            remapped_mask = mask\n","\n","        one_hot_mask = tf.keras.utils.to_categorical(remapped_mask, num_classes=self.num_classes)\n","        return one_hot_mask.astype('float32')\n","\n","    def compute_class_weights(self):\n","        \"\"\"Calcule les poids pour chaque classe et les logue dans MLFlow.\"\"\"\n","        pixel_counts = np.zeros(self.num_classes, dtype=np.int64)\n","\n","        for mask_path in self.mask_paths:\n","            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","            mask = cv2.resize(mask, self.img_size[::-1], interpolation=cv2.INTER_NEAREST)\n","\n","            for cls, grp in self.class_to_group.items():\n","                pixel_counts[grp] += np.sum(mask == cls)\n","\n","        total_pixels = np.sum(pixel_counts)\n","        class_weights = total_pixels / (self.num_classes * pixel_counts)\n","        normalized_weights = class_weights / np.sum(class_weights)\n","\n","        # Loguer les poids des classes dans MLFlow\n","        for i, weight in enumerate(normalized_weights):\n","            mlflow.log_param(f\"class_weight_{i}\", float(weight))\n","\n","        return normalized_weights"]},{"cell_type":"markdown","metadata":{"id":"1by_vFUzDD2c"},"source":["##### <font color='blue'>4.3 Charger le pipeline d'augmentation et initialiser les générateurs</font><a class=\"anchor\" id=\"partie4.3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5BNKhGaDLL2"},"outputs":[],"source":["with mlflow.start_run(run_name=\"Preprocessing\"):\n","    # Charger les pipelines d'augmentation et les loguer dans MLFlow\n","    augmentation_pipeline = get_augmentation_pipeline()\n","\n","    # Initialiser les générateurs\n","    train_gen = DataGenerator(\n","        train_images,\n","        train_masks,\n","        batch_size=16,\n","        img_size=(256, 256),\n","        augmentation=augmentation_pipeline,\n","        class_to_group=class_to_group,\n","        num_classes=8\n","    )\n","\n","    val_gen = DataGenerator(\n","        val_images,\n","        val_masks,\n","        batch_size=16,\n","        img_size=(256, 256),\n","        class_to_group=class_to_group,\n","        num_classes=8\n","    )\n","\n","    # Loguer la taille des images dans MLFlow\n","    mlflow.log_param(\"generator_img_size\", train_gen.img_size)\n","\n","    # Calculer et loguer les poids des classes\n","    class_weights = train_gen.compute_class_weights()\n","    mlflow.log_dict({\"class_weights\": class_weights.tolist()}, \"class_weights.json\")"]},{"cell_type":"markdown","metadata":{"id":"lPxXABtZZ0AU"},"source":["## <font color='red'>5. Modélisation</font><a class=\"anchor\" id=\"partie5\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ClCSGU7exjk4"},"source":["###### <font color='purple'>Fonctions de perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7VVjaUYRGg8"},"outputs":[],"source":["from tensorflow.keras.losses import binary_crossentropy\n","\n","def total_loss(y_true, y_pred):\n","    \"\"\"\n","    Combine binary_crossentropy et dice_loss pour améliorer les performances globales.\n","\n","    Args:\n","        y_true (Tensor): Masques réels.\n","        y_pred (Tensor): Masques prédits.\n","\n","    Returns:\n","        Tensor: Valeur de la perte combinée.\n","    \"\"\"\n","    loss = binary_crossentropy(y_true, y_pred) + (3 * dice_loss(y_true, y_pred))\n","    return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuaiW4lm7NMq"},"outputs":[],"source":["def dice_coeff(y_true, y_pred):\n","    \"\"\"\n","    Calcule le coefficient de Dice, une mesure de similarité pour la segmentation.\n","\n","    Args:\n","        y_true (Tensor): Masques vrais (ground truth).\n","        y_pred (Tensor): Masques prédits.\n","\n","    Returns:\n","        float: Coefficient de Dice.\n","    \"\"\"\n","    smooth = 1.0\n","    y_true_f = tf.keras.backend.flatten(y_true)\n","    y_pred_f = tf.keras.backend.flatten(y_pred)\n","    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)"]},{"cell_type":"markdown","metadata":{"id":"gn5jsOy29kKk"},"source":["##### <font color='blue'>5.1 Modèle U-Net Mini</font><a class=\"anchor\" id=\"partie5.1\"></a>"]},{"cell_type":"markdown","source":["Le modèle U-Net Mini sert de baseline dans cette étude. Il conserve la structure classique d'un U-Net (encodeur-décodeur avec connexions directes) mais avec un nombre réduit de paramètres, ce qui en fait une option légère et rapide à entraîner. Il est idéal pour évaluer les performances de base avant de tester des modèles plus complexes.\n","\n"],"metadata":{"id":"9EvbELJ35vWr"}},{"cell_type":"markdown","metadata":{"id":"HFliVYgW9xfK"},"source":["###### <font color='green'>Conception du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1435,"status":"ok","timestamp":1736958377383,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"8H0h5tAf91ZZ","outputId":"29fca29f-04a7-4d81-dd5c-545a970579cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Modèle U-Net Mini conçu et loggé dans MLFlow en 1.13 secondes.\n"]}],"source":["def unet_mini(input_size=(256, 256, 3), n_classes=32):\n","    \"\"\"\n","    Implémente un modèle U-Net simplifié avec des convolutions dilatées.\n","\n","    Args:\n","        input_size (tuple): Dimensions des images d'entrée (par défaut 256x256x3).\n","        n_classes (int): Nombre de classes pour la segmentation (par défaut 8).\n","\n","    Returns:\n","        Model: Modèle U-Net Mini.\n","    \"\"\"\n","    inputs = Input(input_size)\n","\n","    # Encodeur\n","    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n","    c1 = BatchNormalization()(c1)\n","    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n","    c1 = BatchNormalization()(c1)\n","    p1 = MaxPooling2D((2, 2))(c1)\n","\n","    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n","    c2 = BatchNormalization()(c2)\n","    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n","    c2 = BatchNormalization()(c2)\n","    p2 = MaxPooling2D((2, 2))(c2)\n","\n","    # Goulot d'étranglement avec convolution dilatée\n","    c3 = Conv2D(128, (3, 3), activation='relu', padding='same', dilation_rate=2)(p2)\n","    c3 = BatchNormalization()(c3)\n","    c3 = Conv2D(128, (3, 3), activation='relu', padding='same', dilation_rate=4)(c3)\n","    c3 = BatchNormalization()(c3)\n","\n","    # Décodeur\n","    u1 = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(c3)\n","    u1 = concatenate([u1, c2])\n","    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n","    c4 = BatchNormalization()(c4)\n","    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n","    c4 = BatchNormalization()(c4)\n","\n","    u2 = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(c4)\n","    u2 = concatenate([u2, c1])\n","    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n","    c5 = BatchNormalization()(c5)\n","    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n","    c5 = BatchNormalization()(c5)\n","\n","    # Couche de sortie\n","    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c5)\n","\n","    return Model(inputs, outputs)\n","\n","with mlflow.start_run(run_name=\"Unet Mini - Conception\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Instancier le modèle\n","    unet_model = unet_mini(input_size=(256, 256, 3), n_classes=8)\n","\n","    # Afficher le résumé du modèle\n","    model_summary = []\n","    unet_model.summary(print_fn=lambda x: model_summary.append(x))\n","    model_summary = \"\\n\".join(model_summary)\n","\n","    # Enregistrer le résumé du modèle dans MLFlow\n","    mlflow.log_text(model_summary, \"unet_mini_summary.txt\")\n","\n","    # Temps de conception du modèle\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"conception_time\", elapsed_time)\n","\n","print(f\"Modèle U-Net Mini conçu et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"PY55f30P94m1"},"source":["###### <font color='green'>Compilateur et Fonctions de Perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1736958377383,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"xjJxaR2B99tf","outputId":"346fe9a0-11f3-41f7-eb2c-1b515191741d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Compilation terminée et loggée dans MLFlow en 0.24 secondes.\n"]}],"source":["# Déclaration des métriques et de la fonction de perte\n","dice_loss = DiceLoss()\n","iou_score = IOUScore(threshold=0.5)\n","f_score = FScore(threshold=0.5)\n","\n","with mlflow.start_run(run_name=\"Unet Mini - Compilation\") as run:\n","    start_time = time.time()\n","\n","    # Compilation du modèle\n","    unet_model.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss=total_loss,\n","        metrics=[iou_score, f_score, dice_coeff]\n","    )\n","\n","    # Log des hyperparamètres\n","    mlflow.log_param(\"Optimizer\", \"Adam\")\n","    mlflow.log_param(\"Learning Rate\", 1e-4)\n","    mlflow.log_param(\"Loss Function\", \"total_loss\")\n","    mlflow.log_param(\"IoU Threshold\", 0.5)\n","    mlflow.log_param(\"FScore Threshold\", 0.5)\n","    mlflow.log_param(\"Dice Coefficient Metric\", \"dice_coeff\")\n","\n","    # Sauvegarde de la configuration du modèle\n","    model_config = unet_model.get_config()\n","    model_config_path = \"unet_mini_config.json\"\n","    with open(model_config_path, \"w\") as f:\n","        json.dump(model_config, f, indent=4)\n","    mlflow.log_artifact(model_config_path)\n","\n","    # Temps d'exécution\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"compilation_time\", elapsed_time)\n","\n","print(f\"Compilation terminée et loggée dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"Vun26Ds5-BRb"},"source":["###### <font color='green'>Entraînement du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416919,"status":"ok","timestamp":1736967352398,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"4-3ZzECK-E0L","outputId":"e037914c-8a7c-47d2-e496-ecdaa28a4696"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","185/185 [==============================] - 477s 3s/step - loss: 2.4380 - iou_score: 0.3397 - f1-score: 0.4375 - dice_coeff: 0.5533 - val_loss: 3.5900 - val_iou_score: 0.0490 - val_f1-score: 0.0716 - val_dice_coeff: 0.3723 - lr: 1.0000e-04\n","Epoch 2/30\n","185/185 [==============================] - 470s 3s/step - loss: 1.8349 - iou_score: 0.4850 - f1-score: 0.5855 - dice_coeff: 0.7512 - val_loss: 3.4705 - val_iou_score: 0.0852 - val_f1-score: 0.1289 - val_dice_coeff: 0.4133 - lr: 1.0000e-04\n","Epoch 3/30\n","185/185 [==============================] - 472s 3s/step - loss: 1.6578 - iou_score: 0.5292 - f1-score: 0.6392 - dice_coeff: 0.7866 - val_loss: 2.4816 - val_iou_score: 0.2814 - val_f1-score: 0.3836 - val_dice_coeff: 0.6412 - lr: 1.0000e-04\n","Epoch 4/30\n","185/185 [==============================] - 471s 3s/step - loss: 1.5290 - iou_score: 0.5550 - f1-score: 0.6693 - dice_coeff: 0.8059 - val_loss: 1.6518 - val_iou_score: 0.5085 - val_f1-score: 0.6297 - val_dice_coeff: 0.7737 - lr: 1.0000e-04\n","Epoch 5/30\n","185/185 [==============================] - 472s 3s/step - loss: 1.4441 - iou_score: 0.5689 - f1-score: 0.6836 - dice_coeff: 0.8189 - val_loss: 1.4407 - val_iou_score: 0.5627 - val_f1-score: 0.6795 - val_dice_coeff: 0.8147 - lr: 1.0000e-04\n","Epoch 6/30\n","185/185 [==============================] - 473s 3s/step - loss: 1.3526 - iou_score: 0.5855 - f1-score: 0.7001 - dice_coeff: 0.8305 - val_loss: 1.3419 - val_iou_score: 0.5814 - val_f1-score: 0.6973 - val_dice_coeff: 0.8283 - lr: 1.0000e-04\n","Epoch 7/30\n","185/185 [==============================] - 474s 3s/step - loss: 1.3093 - iou_score: 0.5890 - f1-score: 0.7039 - dice_coeff: 0.8339 - val_loss: 1.3153 - val_iou_score: 0.5789 - val_f1-score: 0.6969 - val_dice_coeff: 0.8347 - lr: 1.0000e-04\n","Epoch 8/30\n","185/185 [==============================] - 471s 3s/step - loss: 1.2394 - iou_score: 0.6008 - f1-score: 0.7150 - dice_coeff: 0.8418 - val_loss: 1.4830 - val_iou_score: 0.5271 - val_f1-score: 0.6592 - val_dice_coeff: 0.7427 - lr: 1.0000e-04\n","Epoch 9/30\n","185/185 [==============================] - 472s 3s/step - loss: 1.1937 - iou_score: 0.6059 - f1-score: 0.7201 - dice_coeff: 0.8451 - val_loss: 1.2246 - val_iou_score: 0.5900 - val_f1-score: 0.7095 - val_dice_coeff: 0.8302 - lr: 1.0000e-04\n","Epoch 10/30\n","185/185 [==============================] - 473s 3s/step - loss: 1.1456 - iou_score: 0.6120 - f1-score: 0.7265 - dice_coeff: 0.8491 - val_loss: 1.1653 - val_iou_score: 0.5999 - val_f1-score: 0.7184 - val_dice_coeff: 0.8405 - lr: 1.0000e-04\n","Epoch 11/30\n","185/185 [==============================] - 470s 3s/step - loss: 1.0964 - iou_score: 0.6196 - f1-score: 0.7335 - dice_coeff: 0.8533 - val_loss: 1.1664 - val_iou_score: 0.5936 - val_f1-score: 0.7125 - val_dice_coeff: 0.8315 - lr: 1.0000e-04\n","Epoch 12/30\n","185/185 [==============================] - 474s 3s/step - loss: 1.0633 - iou_score: 0.6236 - f1-score: 0.7374 - dice_coeff: 0.8538 - val_loss: 1.1468 - val_iou_score: 0.5913 - val_f1-score: 0.7132 - val_dice_coeff: 0.8297 - lr: 1.0000e-04\n","Epoch 13/30\n","185/185 [==============================] - 472s 3s/step - loss: 1.0411 - iou_score: 0.6242 - f1-score: 0.7382 - dice_coeff: 0.8539 - val_loss: 1.0294 - val_iou_score: 0.6250 - val_f1-score: 0.7391 - val_dice_coeff: 0.8545 - lr: 1.0000e-04\n","Epoch 14/30\n","185/185 [==============================] - 472s 3s/step - loss: 1.0073 - iou_score: 0.6305 - f1-score: 0.7424 - dice_coeff: 0.8594 - val_loss: 1.0645 - val_iou_score: 0.6102 - val_f1-score: 0.7270 - val_dice_coeff: 0.8431 - lr: 1.0000e-04\n","Epoch 15/30\n","185/185 [==============================] - 472s 3s/step - loss: 0.9832 - iou_score: 0.6328 - f1-score: 0.7453 - dice_coeff: 0.8605 - val_loss: 1.0253 - val_iou_score: 0.6171 - val_f1-score: 0.7338 - val_dice_coeff: 0.8450 - lr: 1.0000e-04\n","Epoch 16/30\n","185/185 [==============================] - 473s 3s/step - loss: 0.9549 - iou_score: 0.6382 - f1-score: 0.7494 - dice_coeff: 0.8647 - val_loss: 0.9703 - val_iou_score: 0.6318 - val_f1-score: 0.7452 - val_dice_coeff: 0.8582 - lr: 1.0000e-04\n","Epoch 17/30\n","185/185 [==============================] - 472s 3s/step - loss: 0.9359 - iou_score: 0.6403 - f1-score: 0.7525 - dice_coeff: 0.8642 - val_loss: 0.9744 - val_iou_score: 0.6262 - val_f1-score: 0.7405 - val_dice_coeff: 0.8576 - lr: 1.0000e-04\n","Epoch 18/30\n","185/185 [==============================] - 470s 3s/step - loss: 0.9257 - iou_score: 0.6411 - f1-score: 0.7531 - dice_coeff: 0.8641 - val_loss: 0.9706 - val_iou_score: 0.6241 - val_f1-score: 0.7407 - val_dice_coeff: 0.8515 - lr: 1.0000e-04\n","Epoch 19/30\n","185/185 [==============================] - ETA: 0s - loss: 0.9034 - iou_score: 0.6457 - f1-score: 0.7565 - dice_coeff: 0.8678\n","Epoch 19: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n","185/185 [==============================] - 472s 3s/step - loss: 0.9034 - iou_score: 0.6457 - f1-score: 0.7565 - dice_coeff: 0.8678 - val_loss: 0.9778 - val_iou_score: 0.6197 - val_f1-score: 0.7360 - val_dice_coeff: 0.8516 - lr: 1.0000e-04\n","Entraînement terminé et loggé dans MLFlow en 8975.11 secondes.\n"]}],"source":["with mlflow.start_run(run_name=\"Unet Mini - Entraînement\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Enregistrement des hyperparamètres\n","    mlflow.log_param(\"model_name\", \"Unet Mini\")\n","    mlflow.log_param(\"epochs\", 30)\n","    mlflow.log_param(\"batch_size\", train_gen.batch_size)\n","    mlflow.log_param(\"image_size\", train_gen.img_size)\n","    mlflow.log_param(\"optimizer\", \"Adam\")\n","    mlflow.log_param(\"learning_rate\", 1e-4)\n","    mlflow.log_param(\"loss_function\", \"total_loss\")\n","    mlflow.log_param(\"iou_score_metric\", \"IOUScore()\")\n","    mlflow.log_param(\"f1_score_metric\", \"FScore(beta=1)\")\n","    mlflow.log_param(\"dice_coeff_metric\", \"dice_coeff\")\n","\n","    # Préparer les callbacks\n","    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_unet_mini\"\n","    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n","    checkpoint_callback = ModelCheckpoint(\n","        \"/content/drive/My Drive/projet 8/Modeles/unet_mini_best_total_loss.h5\",\n","        save_best_only=True,\n","        monitor=\"val_loss\",\n","        mode=\"min\"\n","    )\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    callbacks = [tensorboard_callback, checkpoint_callback, early_stopping, reduce_lr]\n","\n","    # Entraînement du modèle\n","    history = unet_model.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=30,\n","        callbacks=callbacks\n","    )\n","\n","    # Enregistrement des métriques par époque\n","    for epoch in range(len(history.history[\"loss\"])):\n","        mlflow.log_metric(\"train_loss\", history.history[\"loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_loss\", history.history[\"val_loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_iou\", history.history[\"iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_iou\", history.history[\"val_iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_f1\", history.history[\"f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_f1\", history.history[\"val_f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_dice_coeff\", history.history[\"dice_coeff\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_dice_coeff\", history.history[\"val_dice_coeff\"][epoch], step=epoch)\n","\n","    # Sauvegarde du modèle final\n","    final_model_path = \"/content/drive/My Drive/projet 8/Modeles/unet_mini_final_total_loss.h5\"\n","    unet_model.save(final_model_path)\n","    mlflow.log_artifact(final_model_path)\n","\n","    # Sauvegarde de l'historique d'entraînement\n","    history_serializable = {key: [float(v) for v in values] for key, values in history.history.items()}\n","    history_path = \"/content/drive/My Drive/projet 8/Modeles/history_unet_mini_total_loss\"\n","    with open(history_path, \"w\") as f:\n","        json.dump(history_serializable, f, indent=4)\n","\n","    mlflow.log_artifact(history_path)\n","\n","    # Temps d'exécution total\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"training_time\", elapsed_time)\n","\n","print(f\"Entraînement terminé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"6UVGlqHG-Ilf"},"source":["###### <font color='green'>Évaluation et Visualisation</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3190,"status":"ok","timestamp":1736967355588,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"gD4xg7zC-MOg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"871bb41b-48a1-4127-c75b-fe41591c8559"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 858ms/step\n","Évaluation terminée et sauvegardée dans MLFlow en 3.27 secondes.\n"]}],"source":["with mlflow.start_run(run_name=\"Unet Mini - Evaluation\") as run:\n","    start_time = time.time()\n","\n","    # Extraire les métriques de l'entraînement\n","    history_dict = history.history\n","    epochs = range(1, len(history_dict.get(\"loss\", [])) + 1)\n","\n","    # Vérification et récupération des métriques\n","    loss_train = history_dict.get(\"loss\", [])\n","    loss_val = history_dict.get(\"val_loss\", [])\n","    iou_train = history_dict.get(\"iou_score\", [])\n","    iou_val = history_dict.get(\"val_iou_score\", [])\n","    f1_train = history_dict.get(\"f1-score\", [])\n","    f1_val = history_dict.get(\"val_f1-score\", [])\n","    dice_train = history_dict.get(\"dice_coeff\", [])\n","    dice_val = history_dict.get(\"val_dice_coeff\", [])\n","\n","    def save_plot(fig, filename):\n","        \"\"\"\n","        Sauvegarde une figure Matplotlib et l'enregistre dans MLFlow.\n","\n","        Args:\n","            fig (matplotlib.figure.Figure): La figure Matplotlib à sauvegarder.\n","            filename (str): Nom du fichier sous lequel l'image sera sauvegardée.\n","        \"\"\"\n","        fig.savefig(filename)\n","        mlflow.log_artifact(filename)\n","        plt.close(fig)\n","        os.remove(filename)  # Nettoyage après logging\n","\n","    # Courbe de la perte\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"loss\", []), label=\"Perte - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_loss\", []), label=\"Perte - Validation\")\n","    ax.set_title(\"Courbe de la perte - U-Net Mini\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Perte\")\n","    ax.legend()\n","    save_plot(fig, \"loss_curve_unet_mini.png\")\n","\n","    # Courbe de l'IoU\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"iou_score\", []), label=\"IoU - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_iou_score\", []), label=\"IoU - Validation\")\n","    ax.set_title(\"Courbe de l'IoU - U-Net Mini\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"IoU\")\n","    ax.legend()\n","    save_plot(fig, \"iou_curve_unet_mini.png\")\n","\n","    # Courbe du Dice Coefficient\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"dice_coeff\", []), label=\"Dice Coefficient - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_dice_coeff\", []), label=\"Dice Coefficient - Validation\")\n","    ax.set_title(\"Courbe du Dice Coefficient - U-Net Mini\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Dice Coefficient\")\n","    ax.legend()\n","    save_plot(fig, \"dice_curve_unet_mini.png\")\n","\n","    # Matrice de confusion\n","    X_val, y_val = val_gen[0]\n","    y_pred = unet_model.predict(X_val)\n","    y_pred_classes = np.argmax(y_pred, axis=-1)\n","    y_true_classes = np.argmax(y_val, axis=-1)\n","\n","    y_pred_flat = y_pred_classes.flatten()\n","    y_true_flat = y_true_classes.flatten()\n","\n","    conf_matrix = confusion_matrix(y_true_flat, y_pred_flat, labels=range(8))\n","    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Void', 'Flat', 'Construction', 'Object', 'Nature', 'Sky', 'Human', 'Vehicle'])\n","\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    disp.plot(ax=ax, cmap=\"viridis\", values_format=\"d\")\n","    plt.xticks(rotation=45)\n","    plt.title(\"Matrice de confusion des groupes - U-Net Mini\")\n","    save_plot(fig, \"confusion_matrix_unet_mini.png\")\n","\n","    # Enregistrement des scores finaux dans MLFlow\n","    final_metrics = {\n","        \"Final Loss\": loss_val[-1] if loss_val else None,\n","        \"Final IoU Score\": iou_val[-1] if iou_val else None,\n","        \"Final F1-Score\": f1_val[-1] if f1_val else None,\n","        \"Final Dice Coefficient\": dice_val[-1] if dice_val else None,\n","    }\n","\n","    for metric, value in final_metrics.items():\n","        if value is not None:\n","            mlflow.log_metric(metric, value)\n","\n","    # Temps d'évaluation total\n","    eval_time = time.time() - start_time\n","    mlflow.log_metric(\"evaluation_time\", eval_time)\n","\n","    print(f\"Évaluation terminée et sauvegardée dans MLFlow en {eval_time:.2f} secondes.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1736967355588,"user":{"displayName":"Samy Louis","userId":"03088254983522727435"},"user_tz":-60},"id":"L9F76zMEYTL_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5ef9fe99-b4d5-481f-8267-4d879dc54ce5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Métriques finales :\n","Perte (Validation) : 0.9778\n","IoU Score (Validation) : 0.6197\n","F1 Score (Validation) : 0.7360\n","Dice Coefficient (Validation) : 0.8516\n"]}],"source":["print(\"Métriques finales :\")\n","print(f\"Perte (Validation) : {history.history['val_loss'][-1]:.4f}\")\n","print(f\"IoU Score (Validation) : {history.history['val_iou_score'][-1]:.4f}\")\n","print(f\"F1 Score (Validation) : {history.history['val_f1-score'][-1]:.4f}\")\n","print(f\"Dice Coefficient (Validation) : {history.history['val_dice_coeff'][-1]:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"keFF9qx4C89U"},"source":["##### <font color='blue'>5.2 Modèle U-Net avec ResNet34</font>"]},{"cell_type":"markdown","source":["Ce modèle intègre ResNet34 comme backbone pour l’encodeur. Grâce à ses connexions résiduelles, ResNet34 améliore la stabilité de l'apprentissage tout en offrant un bon équilibre entre précision et efficacité. Le décodeur utilise les caractéristiques extraites pour reconstruire les masques segmentés."],"metadata":{"id":"lz3HmdXk520k"}},{"cell_type":"markdown","metadata":{"id":"roRX92yGEq0l"},"source":["###### <font color='green'>Conception du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qt_8UBQhEtBA"},"outputs":[],"source":["with mlflow.start_run(run_name=\"U-Net ResNet34 - Conception\") as run:\n","    start_time = time.time()  # Démarrer le chronomètre\n","\n","    # Définition du modèle U-Net avec ResNet34\n","    unet_resnet34 = sm.Unet(\n","        backbone_name=\"resnet34\",\n","        input_shape=(256, 256, 3),\n","        classes=8,\n","        activation=\"softmax\"\n","    )\n","\n","    # Afficher et enregistrer le résumé du modèle\n","    model_summary = []\n","    unet_resnet34.summary(print_fn=lambda x: model_summary.append(x))\n","    model_summary_str = \"\\n\".join(model_summary)\n","\n","    # Enregistrer le résumé dans MLFlow directement\n","    mlflow.log_text(model_summary_str, \"unet_resnet34_summary.txt\")\n","\n","    # Enregistrer les paramètres et le temps de conception\n","    mlflow.log_param(\"Backbone\", \"ResNet34\")  # Enregistrer le backbone utilisé\n","    elapsed_time = time.time() - start_time  # Temps de conception\n","    mlflow.log_metric(\"conception_time\", elapsed_time)\n","\n","    print(f\"Modèle U-Net ResNet34 conçu et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"x5_Ba_-YEzpS"},"source":["###### <font color='green'>Compilateur et Fonctions de Perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Row_Rr07E5B4"},"outputs":[],"source":["with mlflow.start_run(run_name=\"U-Net ResNet34 - Compilation\"):\n","    start_time = time.time()  # Chrono\n","\n","    # Fonction de perte et métriques\n","    dice_loss = sm.losses.DiceLoss()\n","    iou_score = sm.metrics.IOUScore(threshold=0.5)\n","    f_score = sm.metrics.FScore(threshold=0.5)\n","\n","    # Compilation du modèle U-Net ResNet34\n","    unet_resnet34.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss=total_loss,\n","        metrics=[iou_score, f_score, dice_coeff]\n","    )\n","\n","    # Log des hyperparamètres\n","    mlflow.log_param(\"Optimizer\", \"Adam\")\n","    mlflow.log_param(\"Learning Rate\", 1e-4)\n","    mlflow.log_param(\"Loss Function\", \"total_loss\")\n","    mlflow.log_param(\"IoU Threshold\", 0.5)\n","    mlflow.log_param(\"FScore Threshold\", 0.5)\n","    mlflow.log_param(\"Dice Coefficient Metric\", \"dice_coeff\")\n","\n","    # Sauvegarde de la configuration du modèle\n","    model_config = unet_resnet34.get_config()\n","    model_config_path = \"unet_resnet34_config.json\"\n","    with open(model_config_path, \"w\") as f:\n","        json.dump(model_config, f, indent=4)\n","    mlflow.log_artifact(model_config_path)\n","\n","    # Temps d'exécution\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"compilation_time\", elapsed_time)\n","\n","print(f\"Compilation terminée et loggée dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"zjvX9Fc9FLIW"},"source":["###### <font color='green'>Entraînement du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcsZ4EXdFQnP"},"outputs":[],"source":["with mlflow.start_run(run_name=\"Unet ResNet34 - Entraînement\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Enregistrement des hyperparamètres\n","    mlflow.log_param(\"epochs\", 30)\n","    mlflow.log_param(\"batch_size\", train_gen.batch_size)\n","    mlflow.log_param(\"image_size\", train_gen.img_size)\n","    mlflow.log_param(\"optimizer\", \"Adam\")\n","    mlflow.log_param(\"learning_rate\", 1e-4)\n","    mlflow.log_param(\"loss_function\", \"total_loss\")\n","    mlflow.log_param(\"iou_score_metric\", \"IOUScore(threshold=0.5)\")\n","    mlflow.log_param(\"f_score_metric\", \"FScore(threshold=0.5)\")\n","    mlflow.log_param(\"dice_coeff_metric\", \"dice_coeff\")\n","\n","    # Préparer les callbacks\n","    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_unet_resnet34\"\n","    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n","    checkpoint_callback = ModelCheckpoint(\n","        \"/content/drive/My Drive/projet 8/Modeles/unet_resnet34_final_total_loss.h5\",\n","        save_best_only=True,\n","        monitor=\"val_loss\",\n","        mode=\"min\"\n","    )\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    callbacks = [tensorboard_callback, checkpoint_callback, early_stopping, reduce_lr]\n","\n","    # Entraînement du modèle\n","    history_resnet34 = unet_resnet34.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=30,\n","        callbacks=callbacks\n","    )\n","\n","    # Enregistrement des métriques par époque dans MLFlow\n","    for epoch in range(len(history_resnet34.history[\"loss\"])):\n","        mlflow.log_metric(\"train_loss\", history_resnet34.history[\"loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_loss\", history_resnet34.history[\"val_loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_iou\", history_resnet34.history[\"iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_iou\", history_resnet34.history[\"val_iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_f1\", history_resnet34.history[\"f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_f1\", history_resnet34.history[\"val_f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_dice_coeff\", history_resnet34.history[\"dice_coeff\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_dice_coeff\", history_resnet34.history[\"val_dice_coeff\"][epoch], step=epoch)\n","\n","    # Sauvegarde du modèle final\n","    final_model_path = \"/content/drive/My Drive/projet 8/Modeles/unet_resnet34_final_total_loss.h5\"\n","    unet_resnet34.save(final_model_path)\n","    mlflow.log_artifact(final_model_path)\n","\n","    # Sauvegarde de l'historique d'entraînement en JSON\n","    history_serializable = {key: [float(v) for v in values] for key, values in history_resnet34.history.items()}\n","    history_path = \"/content/drive/My Drive/projet 8/Modeles/history_unet_resnet34_total_loss.json\"\n","    with open(history_path, \"w\") as f:\n","        json.dump(history_serializable, f, indent=4)\n","\n","    mlflow.log_artifact(history_path)\n","\n","    # Temps d'exécution total\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"training_time\", elapsed_time)\n","\n","print(f\"Entraînement terminé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"pD_3IQSmFWWh"},"source":["###### <font color='green'>Évaluation et Visualisation</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlNKFWfmNZa5"},"outputs":[],"source":["with mlflow.start_run(run_name=\"U-Net ResNet34 - Evaluation\") as run:\n","    start_time = time.time()\n","\n","    # Extraire les métriques d'entraînement\n","    history_dict = history_resnet34.history\n","    epochs = range(1, len(history_dict.get(\"loss\", [])) + 1)\n","\n","    # Vérification et récupération des métriques\n","    loss_train = history_dict.get(\"loss\", [])\n","    loss_val = history_dict.get(\"val_loss\", [])\n","    iou_train = history_dict.get(\"iou_score\", [])\n","    iou_val = history_dict.get(\"val_iou_score\", [])\n","    f1_train = history_dict.get(\"f1-score\", [])\n","    f1_val = history_dict.get(\"val_f1-score\", [])\n","    dice_train = history_dict.get(\"dice_coeff\", [])\n","    dice_val = history_dict.get(\"val_dice_coeff\", [])\n","\n","    def save_plot(fig, filename):\n","        \"\"\"\n","        Sauvegarde une figure Matplotlib et l'enregistre dans MLFlow.\n","\n","        Args:\n","            fig (matplotlib.figure.Figure): La figure Matplotlib à sauvegarder.\n","            filename (str): Nom du fichier sous lequel l'image sera sauvegardée.\n","        \"\"\"\n","        fig.savefig(filename)\n","        mlflow.log_artifact(filename)\n","        plt.close(fig)\n","        os.remove(filename)  # Nettoyage après logging\n","\n","    # Courbe de la perte\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"loss\", []), label=\"Perte - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_loss\", []), label=\"Perte - Validation\")\n","    ax.set_title(\"Courbe de la perte - U-Net ResNet34\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Perte\")\n","    ax.legend()\n","    save_plot(fig, \"loss_curve_unet_resnet34.png\")\n","\n","    # Courbe de l'IoU\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"iou_score\", []), label=\"IoU - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_iou_score\", []), label=\"IoU - Validation\")\n","    ax.set_title(\"Courbe de l'IoU - U-Net ResNet34\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"IoU\")\n","    ax.legend()\n","    save_plot(fig, \"iou_curve_unet_resnet34.png\")\n","\n","    # Courbe du Dice Coefficient\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"dice_coeff\", []), label=\"Dice Coefficient - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_dice_coeff\", []), label=\"Dice Coefficient - Validation\")\n","    ax.set_title(\"Courbe du Dice Coefficient - U-Net ResNet34\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Dice Coefficient\")\n","    ax.legend()\n","    save_plot(fig, \"dice_curve_unet_resnet34.png\")\n","\n","    # Matrice de confusion\n","    X_val, y_val = val_gen[0]\n","    y_pred = unet_resnet34.predict(X_val)\n","    y_pred_classes = np.argmax(y_pred, axis=-1)\n","    y_true_classes = np.argmax(y_val, axis=-1)\n","\n","    y_pred_flat = y_pred_classes.flatten()\n","    y_true_flat = y_true_classes.flatten()\n","\n","    conf_matrix = confusion_matrix(y_true_flat, y_pred_flat, labels=range(8))\n","    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Void', 'Flat', 'Construction', 'Object', 'Nature', 'Sky', 'Human', 'Vehicle'])\n","\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    disp.plot(ax=ax, cmap=\"viridis\", values_format=\"d\")\n","    plt.xticks(rotation=45)\n","    plt.title(\"Matrice de confusion des groupes - U-Net ResNet34\")\n","    save_plot(fig, \"confusion_matrix_unet_resnet34.png\")\n","\n","    # Enregistrement des scores finaux dans MLFlow\n","    final_metrics = {\n","        \"Final Loss\": loss_val[-1] if loss_val else None,\n","        \"Final IoU Score\": iou_val[-1] if iou_val else None,\n","        \"Final F1-Score\": f1_val[-1] if f1_val else None,\n","        \"Final Dice Coefficient\": dice_val[-1] if dice_val else None,\n","    }\n","\n","    for metric, value in final_metrics.items():\n","        if value is not None:\n","            mlflow.log_metric(metric, value)\n","\n","    # Temps d'évaluation total\n","    eval_time = time.time() - start_time\n","    mlflow.log_metric(\"evaluation_time\", eval_time)\n","\n","    print(f\"Évaluation terminée et sauvegardée dans MLFlow en {eval_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"mfzJ7ft8xtFW"},"source":["##### <font color='blue'>5.3 Modèle U-Net avec EfficientNetB0</font>"]},{"cell_type":"markdown","source":["En utilisant EfficientNetB0 comme backbone, ce modèle mise sur une architecture optimisée pour l'efficacité et la précision. EfficientNetB0 utilise des techniques avancées de mise à l'échelle pour capturer des caractéristiques riches tout en limitant la complexité computationnelle.\n","\n"],"metadata":{"id":"j2jXR1hw56ek"}},{"cell_type":"markdown","metadata":{"id":"skh2MWK7x3_u"},"source":["###### <font color='green'>Conception du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfLLu6JEx9so"},"outputs":[],"source":["with mlflow.start_run(run_name=\"U-Net EfficientNetB0 - Conception\") as run:\n","    start_time = time.time()  # Début du chrono\n","\n","    # Définition du modèle U-Net avec EfficientNetB0 comme backbone\n","    unet_efficientnet = sm.Unet(\n","        backbone_name=\"efficientnetb0\",\n","        input_shape=(256, 256, 3),\n","        classes=8,\n","        activation=\"softmax\"\n","    )\n","\n","    # Générer le résumé du modèle\n","    model_summary = []\n","    unet_efficientnet.summary(print_fn=lambda x: model_summary.append(x))\n","    model_summary_str = \"\\n\".join(model_summary)\n","\n","    # Enregistrement du résumé dans MLFlow\n","    summary_path = \"unet_efficientnet_summary.txt\"\n","    with open(summary_path, \"w\") as f:\n","        f.write(model_summary_str)\n","\n","    mlflow.log_artifact(summary_path)  # Sauvegarde dans MLFlow\n","    mlflow.log_param(\"Backbone\", \"EfficientNetB0\")  # Enregistrer le backbone utilisé\n","\n","    # Mesurer le temps de conception\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"conception_time\", elapsed_time)\n","\n","    print(f\"Modèle U-Net EfficientNetB0 conçu et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"mhr5iGTfyA7o"},"source":["###### <font color='green'>Compilateur et Fonctions de Perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6l7iJZv92TJL"},"outputs":[],"source":["with mlflow.start_run(run_name=\"U-Net EfficientNetB0 - Compilation\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Déclaration des métriques et de la fonction de perte\n","    dice_loss = sm.losses.DiceLoss()\n","    iou_score = sm.metrics.IOUScore(threshold=0.5)\n","    f_score = sm.metrics.FScore(threshold=0.5)\n","\n","    # Compilation du modèle\n","    unet_efficientnet.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss=total_loss,\n","        metrics=[iou_score, f_score, dice_coeff]\n","    )\n","\n","    # Enregistrement des hyperparamètres\n","    mlflow.log_param(\"Optimizer\", \"Adam\")\n","    mlflow.log_param(\"Learning Rate\", 1e-4)\n","    mlflow.log_param(\"Loss Function\", \"total_loss\")\n","    mlflow.log_param(\"IoU Threshold\", 0.5)\n","    mlflow.log_param(\"FScore Threshold\", 0.5)\n","    mlflow.log_param(\"Dice Coefficient Metric\", \"dice_coeff\")\n","\n","    # Sauvegarde de la configuration du modèle\n","    model_config = unet_efficientnet.get_config()\n","    config_path = \"unet_efficientnet_config.json\"\n","    with open(config_path, \"w\") as f:\n","        json.dump(model_config, f, indent=4)\n","\n","    mlflow.log_artifact(config_path)\n","\n","    # Temps de compilation\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"compilation_time\", elapsed_time)\n","\n","    print(f\"Modèle U-Net EfficientNetB0 compilé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"QF7XWWQz2pGo"},"source":["###### <font color='green'>Entraînement du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yq-Ezt628i2"},"outputs":[],"source":["with mlflow.start_run(run_name=\"U-Net EfficientNetB0 - Entraînement\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Préparer les callbacks\n","    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_unet_efficientnet\"\n","    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n","    checkpoint_callback = ModelCheckpoint(\n","        \"/content/drive/My Drive/projet 8/Modeles/unet_efficientnet_best_total_loss.h5\",\n","        save_best_only=True,\n","        monitor=\"val_loss\",\n","        mode=\"min\"\n","    )\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    callbacks = [tensorboard_callback, checkpoint_callback, early_stopping, reduce_lr]\n","\n","    # Entraînement du modèle\n","    history_efficientnet = unet_efficientnet.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=30,\n","        callbacks=callbacks\n","    )\n","\n","    # Enregistrement des métriques par époque\n","    for epoch in range(len(history_efficientnet.history[\"loss\"])):\n","        mlflow.log_metric(\"train_loss\", history_efficientnet.history[\"loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_loss\", history_efficientnet.history[\"val_loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_iou\", history_efficientnet.history[\"iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_iou\", history_efficientnet.history[\"val_iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_f1\", history_efficientnet.history[\"f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_f1\", history_efficientnet.history[\"val_f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_dice_coeff\", history_efficientnet.history[\"dice_coeff\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_dice_coeff\", history_efficientnet.history[\"val_dice_coeff\"][epoch], step=epoch)\n","\n","    # Sauvegarde du modèle final\n","    final_model_path = \"/content/drive/My Drive/projet 8/Modeles//unet_efficientnet_final_total_loss.h5\"\n","    unet_efficientnet.save(final_model_path)\n","    mlflow.log_artifact(final_model_path)\n","\n","    # Sauvegarde de l'historique d'entraînement\n","    history_path = \"/content/drive/My Drive/projet 8/Modeles//history_unet_efficientnet_total_loss.json\"\n","    history_serializable = {key: [float(v) for v in values] for key, values in history_efficientnet.history.items()}\n","    with open(history_path, \"w\") as f:\n","        json.dump(history_serializable, f, indent=4)\n","\n","    mlflow.log_artifact(history_path)\n","\n","    # Temps d'exécution total\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"training_time\", elapsed_time)\n","\n","    print(f\"Entraînement terminé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"TqcAmAIo2trI"},"source":["###### <font color='green'>Évaluation et Visualisation</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJvUbVm42uE6"},"outputs":[],"source":["with mlflow.start_run(run_name=\"U-Net EfficientNetB0 - Évaluation\") as run:\n","    start_time = time.time()\n","\n","    # Extraire les métriques d'entraînement\n","    history_dict = history_efficientnet.history\n","    epochs = range(1, len(history_dict.get(\"loss\", [])) + 1)\n","\n","    # Vérification et récupération des métriques\n","    loss_train = history_dict.get(\"loss\", [])\n","    loss_val = history_dict.get(\"val_loss\", [])\n","    iou_train = history_dict.get(\"iou_score\", [])\n","    iou_val = history_dict.get(\"val_iou_score\", [])\n","    f1_train = history_dict.get(\"f1-score\", [])\n","    f1_val = history_dict.get(\"val_f1-score\", [])\n","    dice_train = history_dict.get(\"dice_coeff\", [])\n","    dice_val = history_dict.get(\"val_dice_coeff\", [])\n","\n","    # Fonction pour sauvegarder les courbes\n","    def save_plot(fig, filename):\n","        \"\"\"\n","        Sauvegarde une figure Matplotlib et l'enregistre dans MLFlow.\n","\n","        Args:\n","            fig (matplotlib.figure.Figure): La figure Matplotlib à sauvegarder.\n","            filename (str): Nom du fichier sous lequel l'image sera sauvegardée.\n","        \"\"\"\n","        fig.savefig(filename)\n","        mlflow.log_artifact(filename)\n","        plt.close(fig)\n","        os.remove(filename)  # Nettoyage après logging\n","\n","    # Courbe de la perte\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"loss\", []), label=\"Perte - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_loss\", []), label=\"Perte - Validation\")\n","    ax.set_title(\"Courbe de la perte - U-Net EfficientNetB0\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Perte\")\n","    ax.legend()\n","    save_plot(fig, \"loss_curve_unet_efficientnet.png\")\n","\n","    # Courbe de l'IoU\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"iou_score\", []), label=\"IoU - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_iou_score\", []), label=\"IoU - Validation\")\n","    ax.set_title(\"Courbe de l'IoU - U-Net EfficientNetB0\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"IoU\")\n","    ax.legend()\n","    save_plot(fig, \"iou_curve_unet_efficientnet.png\")\n","\n","    # Courbe du Dice Coefficient\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, history_dict.get(\"dice_coeff\", []), label=\"Dice Coefficient - Entraînement\")\n","    ax.plot(epochs, history_dict.get(\"val_dice_coeff\", []), label=\"Dice Coefficient - Validation\")\n","    ax.set_title(\"Courbe du Dice Coefficient - U-Net EfficientNetB0\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Dice Coefficient\")\n","    ax.legend()\n","    save_plot(fig, \"dice_curve_unet_efficientnet.png\")\n","\n","    # Matrice de confusion\n","    X_val, y_val = val_gen[0]\n","    y_pred = unet_efficientnet.predict(X_val)\n","    y_pred_classes = np.argmax(y_pred, axis=-1)\n","    y_true_classes = np.argmax(y_val, axis=-1)\n","\n","    y_pred_flat = y_pred_classes.flatten()\n","    y_true_flat = y_true_classes.flatten()\n","\n","    conf_matrix = confusion_matrix(y_true_flat, y_pred_flat, labels=range(8))\n","    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Void', 'Flat', 'Construction', 'Object', 'Nature', 'Sky', 'Human', 'Vehicle'])\n","\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    disp.plot(ax=ax, cmap=\"viridis\", values_format=\"d\")\n","    plt.xticks(rotation=45)\n","    plt.title(\"Matrice de confusion des groupes - U-Net EfficientNetB0\")\n","    save_plot(fig, \"confusion_matrix_unet_efficientnet.png\")\n","\n","    # Enregistrement des scores finaux dans MLFlow\n","    final_metrics = {\n","        \"Final Loss\": loss_val[-1] if loss_val else None,\n","        \"Final IoU Score\": iou_val[-1] if iou_val else None,\n","        \"Final F1-Score\": f1_val[-1] if f1_val else None,\n","        \"Final Dice Coefficient\": dice_val[-1] if dice_val else None,\n","    }\n","\n","    for metric, value in final_metrics.items():\n","        if value is not None:\n","            mlflow.log_metric(metric, value)\n","\n","    # Temps d'évaluation total\n","    eval_time = time.time() - start_time\n","    mlflow.log_metric(\"evaluation_time\", eval_time)\n","\n","    print(f\"Évaluation terminée et sauvegardée dans MLFlow en {eval_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"ZJcmk1siuy9p"},"source":["##### <font color='blue'>5.4 Modèle SegNet avec VGG16</font>"]},{"cell_type":"markdown","source":["SegNet utilise VGG16 comme backbone et applique un décodeur basé sur des indices de pooling, récupérant ainsi des informations importantes des étapes d'encodage. Bien que simple et efficace, l’absence de connexions directes entre encodeur et décodeur peut limiter sa capacité à capturer des détails fins."],"metadata":{"id":"ErLyEg9P58Zs"}},{"cell_type":"markdown","metadata":{"id":"Pmx3nJ37u9uO"},"source":["###### <font color='green'>Conception du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilbOoX0eu9ie"},"outputs":[],"source":["def segnet(input_size=(256, 256, 3), n_classes=8):\n","    \"\"\"\n","    Implémente le modèle SegNet avec un encodeur basé sur VGG16 et un décodeur symétrique.\n","\n","    Args:\n","        input_size (tuple): Dimensions des images en entrée (par défaut 256x256x3).\n","        n_classes (int): Nombre de classes de segmentation.\n","\n","    Returns:\n","        Model: Modèle SegNet construit.\n","    \"\"\"\n","    inputs = Input(input_size)\n","\n","    # ENCODEUR (Basé sur VGG16)\n","    c1 = Conv2D(64, (3, 3), padding='same')(inputs)\n","    c1 = BatchNormalization()(c1)\n","    c1 = Activation('relu')(c1)\n","    c1 = Conv2D(64, (3, 3), padding='same')(c1)\n","    c1 = BatchNormalization()(c1)\n","    c1 = Activation('relu')(c1)\n","    p1 = MaxPooling2D((2, 2), strides=(2, 2))(c1)\n","\n","    c2 = Conv2D(128, (3, 3), padding='same')(p1)\n","    c2 = BatchNormalization()(c2)\n","    c2 = Activation('relu')(c2)\n","    c2 = Conv2D(128, (3, 3), padding='same')(c2)\n","    c2 = BatchNormalization()(c2)\n","    c2 = Activation('relu')(c2)\n","    p2 = MaxPooling2D((2, 2), strides=(2, 2))(c2)\n","\n","    c3 = Conv2D(256, (3, 3), padding='same')(p2)\n","    c3 = BatchNormalization()(c3)\n","    c3 = Activation('relu')(c3)\n","    c3 = Conv2D(256, (3, 3), padding='same')(c3)\n","    c3 = BatchNormalization()(c3)\n","    c3 = Activation('relu')(c3)\n","    p3 = MaxPooling2D((2, 2), strides=(2, 2))(c3)\n","\n","    # DECODEUR\n","    u3 = UpSampling2D((2, 2))(p3)\n","    c4 = Conv2D(256, (3, 3), padding='same')(u3)\n","    c4 = BatchNormalization()(c4)\n","    c4 = Activation('relu')(c4)\n","    c4 = Conv2D(256, (3, 3), padding='same')(c4)\n","    c4 = BatchNormalization()(c4)\n","    c4 = Activation('relu')(c4)\n","\n","    u2 = UpSampling2D((2, 2))(c4)\n","    c5 = Conv2D(128, (3, 3), padding='same')(u2)\n","    c5 = BatchNormalization()(c5)\n","    c5 = Activation('relu')(c5)\n","    c5 = Conv2D(128, (3, 3), padding='same')(c5)\n","    c5 = BatchNormalization()(c5)\n","    c5 = Activation('relu')(c5)\n","\n","    u1 = UpSampling2D((2, 2))(c5)\n","    c6 = Conv2D(64, (3, 3), padding='same')(u1)\n","    c6 = BatchNormalization()(c6)\n","    c6 = Activation('relu')(c6)\n","    c6 = Conv2D(64, (3, 3), padding='same')(c6)\n","    c6 = BatchNormalization()(c6)\n","    c6 = Activation('relu')(c6)\n","\n","    # COUCHE DE SORTIE\n","    outputs = Conv2D(n_classes, (1, 1), activation=\"softmax\")(c6)\n","\n","    return Model(inputs, outputs)\n","\n","# Démarrer un run MLFlow pour la conception du modèle\n","with mlflow.start_run(run_name=\"SegNet - Conception\") as run:\n","    start_time = time.time()\n","\n","    # Instancier le modèle\n","    segnet_model = segnet(input_size=(256, 256, 3), n_classes=8)\n","\n","    # Afficher et enregistrer le résumé du modèle\n","    model_summary = []\n","    segnet_model.summary(print_fn=lambda x: model_summary.append(x))\n","    model_summary_str = \"\\n\".join(model_summary)\n","\n","    # Sauvegarde du résumé dans MLFlow\n","    mlflow.log_text(model_summary_str, \"segnet_summary.txt\")\n","\n","    # Temps de conception du modèle\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"model_conception_time_seconds\", elapsed_time)\n","\n","print(f\"Modèle SegNet conçu et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"AfJTmW-Au9DF"},"source":["###### <font color='green'>Compilateur et Fonctions de Perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMzdDLV0u64P"},"outputs":[],"source":["# Démarrer un run MLFlow pour la compilation du modèle\n","with mlflow.start_run(run_name=\"SegNet - Compilation\") as run:\n","    start_time = time.time()\n","\n","    # Définition des métriques et de la fonction de perte\n","    dice_loss = sm.losses.DiceLoss()\n","    iou_score = sm.metrics.IOUScore(threshold=0.5)\n","    f_score = sm.metrics.FScore(threshold=0.5)\n","\n","    # Compilation du modèle SegNet\n","    segnet_model.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss=total_loss,\n","        metrics=[iou_score, f_score, dice_coeff]\n","    )\n","\n","    # Log des hyperparamètres\n","    mlflow.log_param(\"Optimizer\", \"Adam\")\n","    mlflow.log_param(\"Learning Rate\", 1e-4)\n","    mlflow.log_param(\"Loss Function\", \"total_loss\")\n","    mlflow.log_param(\"IoU Threshold\", 0.5)\n","    mlflow.log_param(\"FScore Threshold\", 0.5)\n","    mlflow.log_param(\"Dice Coefficient Metric\", \"dice_coeff\")\n","\n","    # Sauvegarde de la configuration du modèle\n","    model_config = segnet_model.get_config()\n","    config_path = \"segnet_config.json\"\n","    with open(config_path, \"w\") as f:\n","        json.dump(model_config, f, indent=4)\n","\n","    mlflow.log_artifact(config_path)\n","\n","    # Temps de compilation\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"Compilation Time\", elapsed_time)\n","\n","print(f\"Compilation du modèle SegNet terminée et loggée dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"OOuQb0lPu6q7"},"source":["###### <font color='green'>Entraînement du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvSkA4Oau6PR"},"outputs":[],"source":["# Démarrer un run MLFlow pour l'entraînement du modèle SegNet\n","with mlflow.start_run(run_name=\"SegNet - Entraînement\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Préparer les callbacks\n","    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_segnet\"\n","    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n","    checkpoint_callback = ModelCheckpoint(\n","        \"/content/drive/My Drive/projet 8/Modeles/segnet_best_total_loss.h5\",\n","        save_best_only=True, monitor=\"val_loss\", mode=\"min\"\n","    )\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    callbacks = [tensorboard_callback, checkpoint_callback, early_stopping, reduce_lr]\n","\n","    # Entraînement du modèle SegNet\n","    history_segnet = segnet_model.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=30,\n","        callbacks=callbacks\n","    )\n","\n","    # Enregistrement des métriques par époque\n","    for epoch in range(len(history_segnet.history[\"loss\"])):\n","        mlflow.log_metric(\"train_loss\", history_segnet.history[\"loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_loss\", history_segnet.history[\"val_loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_iou\", history_segnet.history[\"iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_iou\", history_segnet.history[\"val_iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_f1\", history_segnet.history[\"f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_f1\", history_segnet.history[\"val_f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_dice_coeff\", history_segnet.history[\"dice_coeff\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_dice_coeff\", history_segnet.history[\"val_dice_coeff\"][epoch], step=epoch)\n","\n","    # Sauvegarde du modèle final\n","    final_model_path = \"/content/drive/My Drive/projet 8/Modeles/segnet_final_total_loss.h5\"\n","    segnet_model.save(final_model_path)\n","    mlflow.log_artifact(final_model_path)\n","\n","    # Sauvegarde de l'historique d'entraînement\n","    history_serializable = {key: [float(v) for v in values] for key, values in history_segnet.history.items()}\n","    history_path = \"/content/drive/My Drive/projet 8/Modeles/history_segnet_total_loss.json\"\n","    with open(history_path, \"w\") as f:\n","        json.dump(history_serializable, f, indent=4)\n","\n","    mlflow.log_artifact(history_path)\n","\n","    # Temps d'exécution total\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"training_time\", elapsed_time)\n","\n","print(f\"Entraînement du modèle SegNet terminé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"iWm-YK_zu6Dg"},"source":["###### <font color='green'>Évaluation et Visualisation</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18F03Q5Nu8FG"},"outputs":[],"source":["# Démarrer un run MLFlow pour l'évaluation du modèle SegNet\n","with mlflow.start_run(run_name=\"SegNet - Evaluation\") as run:\n","    start_time = time.time()\n","\n","    # Extraire les métriques d'entraînement\n","    history_dict = history_segnet.history\n","    epochs = range(1, len(history_dict.get(\"loss\", [])) + 1)\n","\n","    # Vérification et récupération des métriques\n","    loss_train = history_dict.get(\"loss\", [])\n","    loss_val = history_dict.get(\"val_loss\", [])\n","    iou_train = history_dict.get(\"iou_score\", [])\n","    iou_val = history_dict.get(\"val_iou_score\", [])\n","    f1_train = history_dict.get(\"f1-score\", [])\n","    f1_val = history_dict.get(\"val_f1-score\", [])\n","    dice_train = history_dict.get(\"dice_coeff\", [])\n","    dice_val = history_dict.get(\"val_dice_coeff\", [])\n","\n","    # Fonction pour sauvegarder et loguer une figure\n","    def save_plot(fig, filename):\n","        \"\"\"\n","        Sauvegarde une figure Matplotlib et l'enregistre dans MLFlow.\n","\n","        Args:\n","            fig (matplotlib.figure.Figure): La figure Matplotlib à sauvegarder.\n","            filename (str): Nom du fichier sous lequel l'image sera sauvegardée.\n","        \"\"\"\n","        fig.savefig(filename)\n","        mlflow.log_artifact(filename)\n","        plt.close(fig)\n","        os.remove(filename)  # Nettoyage après logging\n","\n","    # Courbe de la perte\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, loss_train, label=\"Perte - Entraînement\")\n","    ax.plot(epochs, loss_val, label=\"Perte - Validation\")\n","    ax.set_title(\"Courbe de la perte - SegNet VGG16\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Perte\")\n","    ax.legend()\n","    save_plot(fig, \"loss_curve_unet_segnet.png\")\n","\n","    # Courbe de l'IoU\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, iou_train, label=\"IoU - Entraînement\")\n","    ax.plot(epochs, iou_val, label=\"IoU - Validation\")\n","    ax.set_title(\"Courbe de l'IoU - SegNet VGG16\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"IoU\")\n","    ax.legend()\n","    save_plot(fig, \"iou_curve_unet_segnet.png\")\n","\n","    # Courbe du Dice Coefficient\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, dice_train, label=\"Dice Coefficient - Entraînement\")\n","    ax.plot(epochs, dice_val, label=\"Dice Coefficient - Validation\")\n","    ax.set_title(\"Courbe du Dice Coefficient - SegNet VGG16\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Dice Coefficient\")\n","    ax.legend()\n","    save_plot(fig, \"dice_curve_unet_segnet.png\")\n","\n","    # Matrice de confusion\n","    X_val, y_val = val_gen[0]\n","    y_pred = segnet_model.predict(X_val)\n","    y_pred_classes = np.argmax(y_pred, axis=-1)\n","    y_true_classes = np.argmax(y_val, axis=-1)\n","\n","    y_pred_flat = y_pred_classes.flatten()\n","    y_true_flat = y_true_classes.flatten()\n","\n","    conf_matrix = confusion_matrix(y_true_flat, y_pred_flat, labels=range(8))\n","    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Void', 'Flat', 'Construction', 'Object', 'Nature', 'Sky', 'Human', 'Vehicle'])\n","\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    disp.plot(ax=ax, cmap=\"viridis\", values_format=\"d\")\n","    plt.xticks(rotation=45)\n","    plt.title(\"Matrice de confusion des groupes - SegNet\")\n","    save_plot(fig, \"confusion_matrix_segnet.png\")\n","\n","    # Enregistrement des scores finaux dans MLFlow\n","    final_metrics = {\n","        \"Final Loss\": loss_val[-1] if loss_val else None,\n","        \"Final IoU Score\": iou_val[-1] if iou_val else None,\n","        \"Final F1-Score\": f1_val[-1] if f1_val else None,\n","        \"Final Dice Coefficient\": dice_val[-1] if dice_val else None,\n","    }\n","\n","    for metric, value in final_metrics.items():\n","        if value is not None:\n","            mlflow.log_metric(metric, value)\n","\n","    # Temps d'évaluation total\n","    eval_time = time.time() - start_time\n","    mlflow.log_metric(\"evaluation_time_seconds\", eval_time)\n","\n","    print(f\"Évaluation du modèle SegNet terminée et sauvegardée dans MLFlow en {eval_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"fHPmq3KPzpnK"},"source":["##### <font color='blue'>5.5 Modèle DeepLabV3+ avec ResNet50</font>"]},{"cell_type":"markdown","source":["Ce modèle exploite ResNet50 comme backbone pour extraire des caractéristiques profondes et intègre un module ASPP (Atrous Spatial Pyramid Pooling) pour capturer des contextes globaux à différentes échelles. Il est particulièrement performant pour les scènes complexes."],"metadata":{"id":"zXEVDsz_6EF8"}},{"cell_type":"markdown","metadata":{"id":"BMgqeTDrz0-G"},"source":["###### <font color='green'>Conception du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-3c-l8bU1X7"},"outputs":[],"source":["def deeplabv3plus(input_shape=(256, 256, 3), num_classes=8):\n","    \"\"\"\n","    Implémente une version simplifiée de DeepLabV3+ avec ResNet50 comme backbone.\n","\n","    Args:\n","        input_shape (tuple): Taille de l'image en entrée.\n","        num_classes (int): Nombre de classes pour la segmentation.\n","\n","    Returns:\n","        keras.Model: Modèle DeepLabV3+.\n","    \"\"\"\n","    # Charger ResNet50 comme Backbone (Sans la dernière couche)\n","    base_model = keras.applications.ResNet50(\n","        weights=\"imagenet\", include_top=False, input_shape=input_shape\n","    )\n","\n","    # Extraire les caractéristiques de la couche 'conv4_block6_out'\n","    x = base_model.get_layer(\"conv4_block6_out\").output\n","\n","    # ASPP (Atrous Spatial Pyramid Pooling)\n","    aspp = layers.Conv2D(256, (3, 3), dilation_rate=6, padding=\"same\", activation=\"relu\")(x)\n","    aspp = layers.BatchNormalization()(aspp)\n","    aspp = layers.Conv2D(256, (3, 3), dilation_rate=12, padding=\"same\", activation=\"relu\")(aspp)\n","    aspp = layers.BatchNormalization()(aspp)\n","    aspp = layers.Conv2D(256, (3, 3), dilation_rate=18, padding=\"same\", activation=\"relu\")(aspp)\n","    aspp = layers.BatchNormalization()(aspp)\n","\n","    # Décodeur\n","    x = layers.Conv2DTranspose(256, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(aspp)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2DTranspose(128, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.UpSampling2D(size=(4, 4), interpolation=\"bilinear\")(x)\n","    x = layers.Conv2D(num_classes, (1, 1), activation=\"softmax\")(x)\n","\n","    # Construire le modèle final\n","    model = keras.Model(inputs=base_model.input, outputs=x)\n","    return model\n","\n","# Définition du modèle\n","with mlflow.start_run(run_name=\"DeepLabV3+ ResNet50 - Conception\") as run:\n","    start_time = time.time()\n","\n","    deeplab_model = deeplabv3plus(input_shape=(256, 256, 3), num_classes=8)\n","\n","    # Afficher et sauvegarder le résumé du modèle\n","    model_summary = []\n","    deeplab_model.summary(print_fn=lambda x: model_summary.append(x))\n","    model_summary_str = \"\\n\".join(model_summary)\n","    mlflow.log_text(model_summary_str, \"deeplabv3plus_resnet50_summary.txt\")\n","\n","    # Temps de conception du modèle\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"model_conception_time_seconds\", elapsed_time)\n","\n","print(f\"Modèle DeepLabV3+ ResNet50 conçu et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mO_erPNrfwoS"},"outputs":[],"source":["print(deeplab_model)"]},{"cell_type":"markdown","metadata":{"id":"fUn2pshqz03K"},"source":["###### <font color='green'>Compilateur et Fonctions de Perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPR4y4F20v5O"},"outputs":[],"source":["# Démarrer le tracking MLFlow pour la compilation du modèle\n","with mlflow.start_run(run_name=\"DeepLabV3+ - Compilation\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Déclaration des métriques et de la fonction de perte\n","    dice_loss = sm.losses.DiceLoss()\n","    iou_score = sm.metrics.IOUScore(threshold=0.5)\n","    f_score = sm.metrics.FScore(threshold=0.5)\n","\n","    # Compilation du modèle\n","    deeplab_model.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss=total_loss,\n","        metrics=[iou_score, f_score, dice_coeff]\n","    )\n","\n","    # Enregistrer les paramètres de compilation dans MLFlow\n","    mlflow.log_param(\"Optimizer\", \"Adam\")\n","    mlflow.log_param(\"Learning Rate\", 1e-4)\n","    mlflow.log_param(\"Loss Function\", \"total_loss\")\n","    mlflow.log_param(\"IoU Threshold\", 0.5)\n","    mlflow.log_param(\"FScore Threshold\", 0.5)\n","    mlflow.log_param(\"Dice Coefficient Metric\", \"dice_coeff\")\n","\n","    # Sauvegarde de la configuration du modèle\n","    model_config = deeplab_model.get_config()\n","    config_path = \"deeplabv3plus_config.json\"\n","    with open(config_path, \"w\") as f:\n","        json.dump(model_config, f, indent=4)\n","\n","    mlflow.log_artifact(config_path)\n","\n","    # Temps de compilation\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"Compilation Time\", elapsed_time)\n","\n","    print(f\"Compilation terminée et loggée dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"pwTkj4KYz0v6"},"source":["###### <font color='green'>Entraînement du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kdi_TIVs1B7I"},"outputs":[],"source":["# Démarrer le tracking MLFlow pour l'entraînement du modèle\n","with mlflow.start_run(run_name=\"DeepLabV3+ - Entraînement\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Préparer les callbacks\n","    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_deeplab_resnet50\"\n","    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n","\n","    model_save_path = \"/content/drive/My Drive/projet 8/Modeles/deeplab_resnet50_best_total_loss.h5\"\n","    checkpoint_callback = ModelCheckpoint(\n","        model_save_path, save_best_only=True, monitor=\"val_loss\", mode=\"min\"\n","    )\n","\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    callbacks = [tensorboard_callback, checkpoint_callback, early_stopping, reduce_lr]\n","\n","    # Enregistrer les hyperparamètres de l'entraînement dans MLFlow\n","    mlflow.log_param(\"Epochs\", 30)\n","    mlflow.log_param(\"Batch Size\", train_gen.batch_size)\n","    mlflow.log_param(\"Image Size\", train_gen.img_size)\n","    mlflow.log_param(\"Callbacks\", [\"EarlyStopping\", \"ModelCheckpoint\", \"ReduceLROnPlateau\"])\n","    mlflow.log_param(\"EarlyStopping Patience\", 3)\n","    mlflow.log_param(\"ReduceLR Factor\", 0.5)\n","    mlflow.log_param(\"ReduceLR Patience\", 3)\n","    mlflow.log_param(\"ReduceLR Min LR\", 1e-6)\n","\n","    # Entraînement du modèle\n","    history_deeplab = deeplab_model.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=30,\n","        callbacks=callbacks\n","    )\n","\n","    # Enregistrement des métriques d’entraînement et validation par époque\n","    for epoch in range(len(history_deeplab.history[\"loss\"])):\n","        mlflow.log_metric(\"train_loss\", history_deeplab.history[\"loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_loss\", history_deeplab.history[\"val_loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_iou\", history_deeplab.history[\"iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_iou\", history_deeplab.history[\"val_iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_f1\", history_deeplab.history[\"f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_f1\", history_deeplab.history[\"val_f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_dice_coeff\", history_deeplab.history[\"dice_coeff\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_dice_coeff\", history_deeplab.history[\"val_dice_coeff\"][epoch], step=epoch)\n","\n","    # Sauvegarde du modèle final\n","    final_model_path = \"/content/drive/My Drive/projet 8/Modeles/deeplab_resnet50_final_total_loss.h5\"\n","    deeplab_model.save(final_model_path)\n","    mlflow.log_artifact(final_model_path)\n","\n","  # Sauvegarde de l'historique d'entraînement\n","    history_serializable = {key: [float(v) for v in values] for key, values in history_deeplab.history.items()}\n","    history_path = \"/content/drive/My Drive/projet 8/Modeles/history_deeplab_resnet50_total_loss.json\"\n","    with open(history_path, \"w\") as f:\n","        json.dump(history_serializable, f, indent=4)\n","\n","    mlflow.log_artifact(history_path)\n","\n","    # Temps d'exécution total\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"training_time\", elapsed_time)\n","\n","    print(f\"Entraînement terminé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"l3KyaK3vz0nO"},"source":["###### <font color='green'>Évaluation et Visualisation</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMYmMx8s-8J8"},"outputs":[],"source":["with mlflow.start_run(run_name=\"DeepLabV3+ ResNet50 - Évaluation\") as run:\n","    start_time = time.time()\n","\n","    # Extraire les métriques d'entraînement\n","    history_dict = history_deeplab.history\n","    epochs = range(1, len(history_dict.get(\"loss\", [])) + 1)\n","\n","    # Vérification et récupération des métriques\n","    loss_train = history_dict.get(\"loss\", [])\n","    loss_val = history_dict.get(\"val_loss\", [])\n","    iou_train = history_dict.get(\"iou_score\", [])\n","    iou_val = history_dict.get(\"val_iou_score\", [])\n","    f1_train = history_dict.get(\"f1-score\", [])\n","    f1_val = history_dict.get(\"val_f1-score\", [])\n","    dice_train = history_dict.get(\"dice_coeff\", [])\n","    dice_val = history_dict.get(\"val_dice_coeff\", [])\n","\n","    # Fonction pour sauvegarder les courbes\n","    def save_plot(fig, filename):\n","        \"\"\"\n","        Sauvegarde une figure Matplotlib et l'enregistre dans MLFlow.\n","\n","        Args:\n","            fig (matplotlib.figure.Figure): La figure Matplotlib à sauvegarder.\n","            filename (str): Nom du fichier sous lequel l'image sera sauvegardée.\n","        \"\"\"\n","        fig.savefig(filename)\n","        mlflow.log_artifact(filename)\n","        plt.close(fig)\n","        os.remove(filename)  # Nettoyage après logging\n","\n","    # Courbe de la perte\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, loss_train, label=\"Perte - Entraînement\")\n","    ax.plot(epochs, loss_val, label=\"Perte - Validation\")\n","    ax.set_title(\"Courbe de la perte - DeepLabV3+ ResNet50\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Perte\")\n","    ax.legend()\n","    save_plot(fig, \"loss_curve_deeplabv3+plus.png\")\n","\n","    # Courbe de l'IoU\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, iou_train, label=\"IoU - Entraînement\")\n","    ax.plot(epochs, iou_val, label=\"IoU - Validation\")\n","    ax.set_title(\"Courbe de l'IoU - DeepLabV3+ ResNet50\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"IoU\")\n","    ax.legend()\n","    save_plot(fig, \"iou_curve_deeplabv3+plus.png\")\n","\n","    # Courbe du Dice Coefficient\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, dice_train, label=\"Dice Coefficient - Entraînement\")\n","    ax.plot(epochs, dice_val, label=\"Dice Coefficient - Validation\")\n","    ax.set_title(\"Courbe du Dice Coefficient - DeepLabV3+ ResNet50\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Dice Coefficient\")\n","    ax.legend()\n","    save_plot(fig, \"dice_curve_deeplabv3+plus.png\")\n","\n","    # Matrice de confusion\n","    X_val, y_val = val_gen[0]\n","    y_pred = deeplab_model.predict(X_val)\n","    y_pred_classes = np.argmax(y_pred, axis=-1)\n","    y_true_classes = np.argmax(y_val, axis=-1)\n","\n","    y_pred_flat = y_pred_classes.flatten()\n","    y_true_flat = y_true_classes.flatten()\n","\n","    conf_matrix = confusion_matrix(y_true_flat, y_pred_flat, labels=range(8))\n","    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Void', 'Flat', 'Construction', 'Object', 'Nature', 'Sky', 'Human', 'Vehicle'])\n","\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    disp.plot(ax=ax, cmap=\"viridis\", values_format=\"d\")\n","    plt.xticks(rotation=45)\n","    plt.title(\"Matrice de confusion des groupes - DeepLabV3+ ResNet50\")\n","    save_plot(fig, \"confusion_matrix_deeplabv3+plus.png\")\n","\n","    # Enregistrement des scores finaux dans MLFlow\n","    final_metrics = {\n","        \"Final Loss\": loss_val[-1] if loss_val else None,\n","        \"Final IoU Score\": iou_val[-1] if iou_val else None,\n","        \"Final F1-Score\": f1_val[-1] if f1_val else None,\n","        \"Final Dice Coefficient\": dice_val[-1] if dice_val else None,\n","    }\n","\n","    for metric, value in final_metrics.items():\n","        if value is not None:\n","            mlflow.log_metric(metric, value)\n","\n","    # Temps d'évaluation total\n","    eval_time = time.time() - start_time\n","    mlflow.log_metric(\"evaluation_time\", eval_time)\n","\n","    print(f\"Évaluation terminée et sauvegardée dans MLFlow en {eval_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"rFf-uYyo0Lgu"},"source":["##### <font color='blue'>5.6 Modèle PSPnet</font>"]},{"cell_type":"markdown","source":["Le modèle PSPNet utilise également ResNet50 comme backbone, mais se distingue par son pooling pyramidal, qui combine des informations locales et globales. Cette approche est idéale pour segmenter des objets de tailles variées dans une scène."],"metadata":{"id":"8Xid_Hxj6GZk"}},{"cell_type":"markdown","metadata":{"id":"eVhsoV7J0Lgv"},"source":["###### <font color='green'>Conception du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPHRCPwHgRJX"},"outputs":[],"source":["def pspnet(input_shape=(288, 288, 3), num_classes=8):  # Changement de 256x256 → 288x288\n","    \"\"\"\n","    Implémente PSPNet avec un backbone ResNet50.\n","\n","    Args:\n","        input_shape (tuple): Taille de l'image en entrée.\n","        num_classes (int): Nombre de classes pour la segmentation.\n","\n","    Returns:\n","        keras.Model: Modèle PSPNet.\n","    \"\"\"\n","    model = sm.PSPNet(\n","        backbone_name=\"resnet50\",\n","        input_shape=input_shape,\n","        classes=num_classes,\n","        activation=\"softmax\"\n","    )\n","    return model\n","\n","# Définition du modèle\n","with mlflow.start_run(run_name=\"PSPNet ResNet50 - Conception\") as run:\n","    start_time = time.time()\n","\n","    pspnet_model = pspnet(input_shape=(288, 288, 3), num_classes=8)  # Taille corrigée\n","\n","    # Afficher et sauvegarder le résumé du modèle\n","    model_summary = []\n","    pspnet_model.summary(print_fn=lambda x: model_summary.append(x))\n","    model_summary_str = \"\\n\".join(model_summary)\n","    mlflow.log_text(model_summary_str, \"pspnet_resnet50_summary.txt\")\n","\n","    # Temps de conception du modèle\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"model_conception_time_seconds\", elapsed_time)\n","\n","print(f\"Modèle PSPNet ResNet50 conçu et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"M-mISQbA0Lgv"},"source":["###### <font color='green'>Compilateur et Fonctions de Perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xqZzNZ5iULZ"},"outputs":[],"source":["# Démarrer le tracking MLFlow pour la compilation du modèle\n","with mlflow.start_run(run_name=\"PSPNet ResNet50 - Compilation\") as run:\n","    start_time = time.time()\n","\n","    # Déclaration des métriques et de la fonction de perte\n","    dice_loss = DiceLoss()\n","    iou_score = IOUScore(threshold=0.5)\n","    f_score = FScore(threshold=0.5)\n","\n","    # Compilation du modèle\n","    pspnet_model.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss=total_loss,\n","        metrics=[iou_score, f_score, dice_coeff]\n","    )\n","\n","    # Enregistrer les paramètres de compilation dans MLFlow\n","    mlflow.log_param(\"Optimizer\", \"Adam\")\n","    mlflow.log_param(\"Learning Rate\", 1e-4)\n","    mlflow.log_param(\"Loss Function\", \"total_loss\")\n","    mlflow.log_param(\"IoU Threshold\", 0.5)\n","    mlflow.log_param(\"FScore Threshold\", 0.5)\n","    mlflow.log_param(\"Dice Coefficient Metric\", \"dice_coeff\")\n","\n","    # Sauvegarde de la configuration du modèle\n","    model_config = pspnet_model.get_config()\n","    config_path = \"pspnet_config.json\"\n","    with open(config_path, \"w\") as f:\n","        json.dump(model_config, f, indent=4)\n","\n","    mlflow.log_artifact(config_path)\n","\n","    # Temps de compilation\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"Compilation Time\", elapsed_time)\n","\n","    print(f\"Compilation terminée et loggée dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"N5X4VNx80Lgv"},"source":["###### <font color='green'>Entraînement du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUwDlq3AiXbM"},"outputs":[],"source":["with mlflow.start_run(run_name=\"PSPNet ResNet50 - Entraînement\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Enregistrement des hyperparamètres\n","    mlflow.log_param(\"model_name\", \"PSPNet ResNet50\")\n","    mlflow.log_param(\"epochs\", 30)\n","    mlflow.log_param(\"batch_size\", train_gen.batch_size)\n","    mlflow.log_param(\"image_size\", train_gen.img_size)\n","\n","    # Préparer les callbacks\n","    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_pspnet\"\n","    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n","    checkpoint_callback = ModelCheckpoint(\n","        \"/content/drive/My Drive/projet 8/Modeles/pspnet_best.h5_total_loss\",\n","        save_best_only=True,\n","        monitor=\"val_loss\",\n","        mode=\"min\"\n","    )\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    callbacks = [tensorboard_callback, checkpoint_callback, early_stopping, reduce_lr]\n","\n","    # Entraînement du modèle\n","    history_pspnet = pspnet_model.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=30,\n","        callbacks=callbacks\n","    )\n","\n","    # Enregistrement des métriques par époque\n","    for epoch in range(len(history_pspnet.history[\"loss\"])):\n","        mlflow.log_metric(\"train_loss\", history_pspnet.history[\"loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_loss\", history_pspnet.history[\"val_loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_iou\", history_pspnet.history[\"iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_iou\", history_pspnet.history[\"val_iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_f1\", history_pspnet.history[\"f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_f1\", history_pspnet.history[\"val_f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_dice_coeff\", history_pspnet.history[\"dice_coeff\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_dice_coeff\", history_pspnet.history[\"val_dice_coeff\"][epoch], step=epoch)\n","\n","    # Sauvegarde du modèle final\n","    final_model_path = \"/content/drive/My Drive/projet 8/Modeles/pspnet_final_total_loss.h5\"\n","    pspnet_model.save(final_model_path)\n","    mlflow.log_artifact(final_model_path)\n","\n","    # Sauvegarde de l'historique d'entraînement\n","    history_serializable = {key: [float(v) for v in values] for key, values in history_pspnet.history.items()}\n","    history_path = \"/content/drive/My Drive/projet 8/Modeles/history_pspnet_total_loss.json\"\n","    with open(history_path, \"w\") as f:\n","        json.dump(history_serializable, f, indent=4)\n","\n","    mlflow.log_artifact(history_path)\n","\n","    # Temps d'exécution total\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"training_time\", elapsed_time)\n","\n","print(f\"Entraînement terminé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"GdiCuakb0Lgv"},"source":["###### <font color='green'>Évaluation et Visualisation</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbX1PLawR_tg"},"outputs":[],"source":["with mlflow.start_run(run_name=\"PSPNet - Évaluation\") as run:\n","    start_time = time.time()\n","\n","    # Extraire les métriques d'entraînement\n","    history_dict = history_pspnet.history\n","    epochs = range(1, len(history_dict.get(\"loss\", [])) + 1)\n","\n","    # Vérification et récupération des métriques\n","    loss_train = history_dict.get(\"loss\", [])\n","    loss_val = history_dict.get(\"val_loss\", [])\n","    iou_train = history_dict.get(\"iou_score\", [])\n","    iou_val = history_dict.get(\"val_iou_score\", [])\n","    f1_train = history_dict.get(\"f1-score\", [])\n","    f1_val = history_dict.get(\"val_f1-score\", [])\n","    dice_train = history_dict.get(\"dice_coeff\", [])\n","    dice_val = history_dict.get(\"val_dice_coeff\", [])\n","\n","    # Fonction pour sauvegarder les courbes\n","    def save_plot(fig, filename):\n","        \"\"\"\n","        Sauvegarde une figure Matplotlib et l'enregistre dans MLFlow.\n","\n","        Args:\n","            fig (matplotlib.figure.Figure): La figure Matplotlib à sauvegarder.\n","            filename (str): Nom du fichier sous lequel l'image sera sauvegardée.\n","        \"\"\"\n","        fig.savefig(filename)\n","        mlflow.log_artifact(filename)\n","        plt.close(fig)\n","        os.remove(filename)  # Nettoyage après logging\n","\n","    # Courbe de la perte\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, loss_train, label=\"Perte - Entraînement\")\n","    ax.plot(epochs, loss_val, label=\"Perte - Validation\")\n","    ax.set_title(\"Courbe de la perte - PSPNet\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Perte\")\n","    ax.legend()\n","    save_plot(fig, \"loss_curve_pspnet.png\")\n","\n","    # Courbe de l'IoU\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, iou_train, label=\"IoU - Entraînement\")\n","    ax.plot(epochs, iou_val, label=\"IoU - Validation\")\n","    ax.set_title(\"Courbe de l'IoU - PSPNet\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"IoU\")\n","    ax.legend()\n","    save_plot(fig, \"iou_curve_pspnet.png\")\n","\n","    # Courbe du Dice Coefficient\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, dice_train, label=\"Dice Coefficient - Entraînement\")\n","    ax.plot(epochs, dice_val, label=\"Dice Coefficient - Validation\")\n","    ax.set_title(\"Courbe du Dice Coefficient - PSPNet\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Dice Coefficient\")\n","    ax.legend()\n","    save_plot(fig, \"dice_curve_pspnet.png\")\n","\n","    # Matrice de confusion\n","    X_val, y_val = val_gen[0]\n","    y_pred = pspnet_model.predict(X_val)\n","    y_pred_classes = np.argmax(y_pred, axis=-1)\n","    y_true_classes = np.argmax(y_val, axis=-1)\n","\n","    y_pred_flat = y_pred_classes.flatten()\n","    y_true_flat = y_true_classes.flatten()\n","\n","    conf_matrix = confusion_matrix(y_true_flat, y_pred_flat, labels=range(8))\n","    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Void', 'Flat', 'Construction', 'Object', 'Nature', 'Sky', 'Human', 'Vehicle'])\n","\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    disp.plot(ax=ax, cmap=\"viridis\", values_format=\"d\")\n","    plt.xticks(rotation=45)\n","    plt.title(\"Matrice de confusion des groupes - PSPNet\")\n","    save_plot(fig, \"confusion_matrix_pspnet.png\")\n","\n","    # Enregistrement des scores finaux dans MLFlow\n","    final_metrics = {\n","        \"Final Loss\": loss_val[-1] if loss_val else None,\n","        \"Final IoU Score\": iou_val[-1] if iou_val else None,\n","        \"Final F1-Score\": f1_val[-1] if f1_val else None,\n","        \"Final Dice Coefficient\": dice_val[-1] if dice_val else None,\n","    }\n","\n","    for metric, value in final_metrics.items():\n","        if value is not None:\n","            mlflow.log_metric(metric, value)\n","\n","    # Temps d'évaluation total\n","    eval_time = time.time() - start_time\n","    mlflow.log_metric(\"evaluation_time\", eval_time)\n","\n","    print(f\"Évaluation terminée et sauvegardée dans MLFlow en {eval_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"By6j4HdE0Mo_"},"source":["##### <font color='blue'>5.7 Modèle FPN</font>"]},{"cell_type":"markdown","source":["Le modèle FPN (Feature Pyramid Network) avec ResNet50 combine les caractéristiques des couches profondes et superficielles du backbone pour obtenir une segmentation équilibrée entre détails fins et contexte global."],"metadata":{"id":"61ici7QB6MAc"}},{"cell_type":"markdown","metadata":{"id":"PDjLlhLI0Mo_"},"source":["###### <font color='green'>Conception du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BbAjl5jTOfp"},"outputs":[],"source":["def fpn_model(input_shape=(512, 512, 3), num_classes=8):\n","    \"\"\"\n","    Implémente le modèle Feature Pyramid Network (FPN) avec ResNet50 comme backbone.\n","\n","    Args:\n","        input_shape (tuple): Taille de l'image en entrée.\n","        num_classes (int): Nombre de classes pour la segmentation.\n","\n","    Returns:\n","        keras.Model: Modèle FPN.\n","    \"\"\"\n","    model = sm.FPN(\n","        backbone_name=\"resnet50\",\n","        input_shape=input_shape,\n","        classes=num_classes,\n","        activation=\"softmax\"\n","    )\n","    return model\n","\n","# Définition du modèle\n","with mlflow.start_run(run_name=\"FPN ResNet50 - Conception\") as run:\n","    start_time = time.time()\n","\n","    fpn_resnet50 = fpn_model(input_shape=(512, 512, 3), num_classes=8)\n","\n","    # Afficher et sauvegarder le résumé du modèle\n","    model_summary = []\n","    fpn_resnet50.summary(print_fn=lambda x: model_summary.append(x))\n","    model_summary_str = \"\\n\".join(model_summary)\n","    mlflow.log_text(model_summary_str, \"fpn_resnet50_summary.txt\")\n","\n","    # Temps de conception du modèle\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"conception_time\", elapsed_time)\n","\n","print(f\"Modèle FPN ResNet50 conçu et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")\n"]},{"cell_type":"markdown","metadata":{"id":"PhviHfR10MpA"},"source":["###### <font color='green'>Compilateur et Fonctions de Perte</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtFHMub9UFDg"},"outputs":[],"source":["# Démarrer le tracking MLFlow pour la compilation du modèle\n","with mlflow.start_run(run_name=\"FPN ResNet50 - Compilation\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Déclaration des métriques et de la fonction de perte\n","    dice_loss = DiceLoss()\n","    iou_score = IOUScore(threshold=0.5)\n","    f_score = FScore(threshold=0.5)\n","\n","    # Compilation du modèle\n","    fpn_resnet50.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss=total_loss,\n","        metrics=[iou_score, f_score, dice_coeff]\n","    )\n","\n","    # Enregistrer les paramètres de compilation dans MLFlow\n","    mlflow.log_param(\"Optimizer\", \"Adam\")\n","    mlflow.log_param(\"Learning Rate\", 1e-4)\n","    mlflow.log_param(\"Loss Function\", \"total_loss\")\n","    mlflow.log_param(\"IoU Threshold\", 0.5)\n","    mlflow.log_param(\"FScore Threshold\", 0.5)\n","    mlflow.log_param(\"Dice Coefficient Metric\", \"dice_coeff\")\n","\n","    # Sauvegarde de la configuration du modèle\n","    model_config = fpn_resnet50.get_config()\n","    config_path = \"fpn_resnet50_config.json\"\n","    with open(config_path, \"w\") as f:\n","        json.dump(model_config, f, indent=4)\n","\n","    mlflow.log_artifact(config_path)\n","\n","    # Temps de compilation\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"Compilation Time\", elapsed_time)\n","\n","    print(f\"Compilation terminée et loggée dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"ga-OkjeD0MpA"},"source":["###### <font color='green'>Entraînement du Modèle</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InFzN8ZfUIiG"},"outputs":[],"source":["with mlflow.start_run(run_name=\"FPN ResNet50 - Entraînement\") as run:\n","    start_time = time.time()  # Début chrono\n","\n","    # Enregistrement des hyperparamètres\n","    mlflow.log_param(\"model_name\", \"FPN ResNet50\")\n","    mlflow.log_param(\"epochs\", 30)\n","    mlflow.log_param(\"batch_size\", train_gen.batch_size)\n","    mlflow.log_param(\"image_size\", train_gen.img_size)\n","    mlflow.log_param(\"optimizer\", \"Adam\")\n","    mlflow.log_param(\"learning_rate\", 1e-4)\n","    mlflow.log_param(\"loss_function\", \"total_loss\")\n","    mlflow.log_param(\"iou_score_metric\", \"IOUScore()\")\n","    mlflow.log_param(\"f1_score_metric\", \"FScore(beta=1)\")\n","    mlflow.log_param(\"dice_coeff_metric\", \"dice_coeff\")\n","\n","    # Préparer les callbacks\n","    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_fpn_resnet50\"\n","    tensorboard_callback = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True)\n","    checkpoint_callback = ModelCheckpoint(\n","        \"/content/drive/My Drive/projet 8/Modeles/fpn_resnet50_best_total_loss.h5\",\n","        save_best_only=True,\n","        monitor=\"val_loss\",\n","        mode=\"min\"\n","    )\n","    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6,\n","        verbose=1\n","    )\n","\n","    callbacks = [tensorboard_callback, checkpoint_callback, early_stopping, reduce_lr]\n","\n","    # Entraînement du modèle\n","    history_fpn = fpn_resnet50.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=30,\n","        callbacks=callbacks\n","    )\n","\n","    # Enregistrement des métriques par époque\n","    for epoch in range(len(history_fpn.history[\"loss\"])):\n","        mlflow.log_metric(\"train_loss\", history_fpn.history[\"loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_loss\", history_fpn.history[\"val_loss\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_iou\", history_fpn.history[\"iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_iou\", history_fpn.history[\"val_iou_score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_f1\", history_fpn.history[\"f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_f1\", history_fpn.history[\"val_f1-score\"][epoch], step=epoch)\n","        mlflow.log_metric(\"train_dice_coeff\", history_fpn.history[\"dice_coeff\"][epoch], step=epoch)\n","        mlflow.log_metric(\"val_dice_coeff\", history_fpn.history[\"val_dice_coeff\"][epoch], step=epoch)\n","\n","    # Sauvegarde du modèle final\n","    final_model_path = \"/content/drive/My Drive/projet 8/Modeles/fpn_resnet50_final_total_loss.h5\"\n","    fpn_resnet50.save(final_model_path)\n","    mlflow.log_artifact(final_model_path)\n","\n","    # Sauvegarde de l'historique d'entraînement\n","    history_serializable = {key: [float(v) for v in values] for key, values in history_fpn.history.items()}\n","    history_path = \"/content/drive/My Drive/projet 8/Modeles/history_fpn_resnet50_total_loss.json\"\n","    with open(history_path, \"w\") as f:\n","        json.dump(history_serializable, f, indent=4)\n","\n","    mlflow.log_artifact(history_path)\n","\n","    # Temps d'exécution total\n","    elapsed_time = time.time() - start_time\n","    mlflow.log_metric(\"training_time\", elapsed_time)\n","\n","    print(f\"Entraînement terminé et loggé dans MLFlow en {elapsed_time:.2f} secondes.\")"]},{"cell_type":"markdown","metadata":{"id":"IQ_qU83y0MpA"},"source":["###### <font color='green'>Évaluation et Visualisation</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26w688xYY-FY"},"outputs":[],"source":["with mlflow.start_run(run_name=\"FPN ResNet50 - Évaluation\") as run:\n","    start_time = time.time()\n","\n","    # Extraire les métriques d'entraînement\n","    history_dict = history_fpn.history\n","    epochs = range(1, len(history_dict.get(\"loss\", [])) + 1)\n","\n","    # Vérification et récupération des métriques\n","    loss_train = history_dict.get(\"loss\", [])\n","    loss_val = history_dict.get(\"val_loss\", [])\n","    iou_train = history_dict.get(\"iou_score\", [])\n","    iou_val = history_dict.get(\"val_iou_score\", [])\n","    f1_train = history_dict.get(\"f1-score\", [])\n","    f1_val = history_dict.get(\"val_f1-score\", [])\n","    dice_train = history_dict.get(\"dice_coeff\", [])\n","    dice_val = history_dict.get(\"val_dice_coeff\", [])\n","\n","    # Fonction pour sauvegarder les courbes\n","    def save_plot(fig, filename):\n","        \"\"\"\n","        Sauvegarde une figure Matplotlib et l'enregistre dans MLFlow.\n","\n","        Args:\n","            fig (matplotlib.figure.Figure): La figure Matplotlib à sauvegarder.\n","            filename (str): Nom du fichier sous lequel l'image sera sauvegardée.\n","        \"\"\"\n","        fig.savefig(filename)\n","        mlflow.log_artifact(filename)\n","        plt.close(fig)\n","        os.remove(filename)  # Nettoyage après logging\n","\n","    # Courbe de la perte\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, loss_train, label=\"Perte - Entraînement\")\n","    ax.plot(epochs, loss_val, label=\"Perte - Validation\")\n","    ax.set_title(\"Courbe de la perte - FPN ResNet50\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Perte\")\n","    ax.legend()\n","    save_plot(fig, \"loss_curve_fpn_resnet50.png\")\n","\n","    # Courbe de l'IoU\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, iou_train, label=\"IoU - Entraînement\")\n","    ax.plot(epochs, iou_val, label=\"IoU - Validation\")\n","    ax.set_title(\"Courbe de l'IoU - FPN ResNet50\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"IoU\")\n","    ax.legend()\n","    save_plot(fig, \"iou_curve_fpn_resnet50.png\")\n","\n","    # Courbe du Dice Coefficient\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ax.plot(epochs, dice_train, label=\"Dice Coefficient - Entraînement\")\n","    ax.plot(epochs, dice_val, label=\"Dice Coefficient - Validation\")\n","    ax.set_title(\"Courbe du Dice Coefficient - FPN ResNet50\")\n","    ax.set_xlabel(\"Époques\")\n","    ax.set_ylabel(\"Dice Coefficient\")\n","    ax.legend()\n","    save_plot(fig, \"dice_curve_fpn_resnet50.png\")\n","\n","    # Matrice de confusion\n","    X_val, y_val = val_gen[0]\n","    y_pred = fpn_resnet50.predict(X_val)\n","    y_pred_classes = np.argmax(y_pred, axis=-1)\n","    y_true_classes = np.argmax(y_val, axis=-1)\n","\n","    y_pred_flat = y_pred_classes.flatten()\n","    y_true_flat = y_true_classes.flatten()\n","\n","    conf_matrix = confusion_matrix(y_true_flat, y_pred_flat, labels=range(8))\n","    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Void', 'Flat', 'Construction', 'Object', 'Nature', 'Sky', 'Human', 'Vehicle'])\n","\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    disp.plot(ax=ax, cmap=\"viridis\", values_format=\"d\")\n","    plt.xticks(rotation=45)\n","    plt.title(\"Matrice de confusion des groupes - FPN ResNet50\")\n","    save_plot(fig, \"confusion_matrix_fpn_resnet50.png\")\n","\n","    # Enregistrement des scores finaux dans MLFlow\n","    final_metrics = {\n","        \"Final Loss\": loss_val[-1] if loss_val else None,\n","        \"Final IoU Score\": iou_val[-1] if iou_val else None,\n","        \"Final F1-Score\": f1_val[-1] if f1_val else None,\n","        \"Final Dice Coefficient\": dice_val[-1] if dice_val else None,\n","    }\n","\n","    for metric, value in final_metrics.items():\n","        if value is not None:\n","            mlflow.log_metric(metric, value)\n","\n","    # Temps d'évaluation total\n","    eval_time = time.time() - start_time\n","    mlflow.log_metric(\"evaluation_time\", eval_time)\n","\n","    print(f\"Évaluation terminée et sauvegardée dans MLFlow en {eval_time:.2f} secondes.\")"]},{"cell_type":"markdown","source":["## <font color='red'>6. Conclusion</font><a class=\"anchor\" id=\"partie6\"></a>"],"metadata":{"id":"XU8iupH5yBvL"}},{"cell_type":"markdown","source":["Après avoir testé plusieurs modèles de segmentation d’images avec différents backbones,\n","le modèle **FPN avec ResNet50** s'est révélé être le plus performant.\n","\n","Résultats clés :\n","- **IoU Score** : 0.770, le plus élevé parmi tous les modèles testés.\n","- **Dice Score** : 0.917, confirmant une segmentation précise.\n","- **Loss** : 0.506, indiquant une bonne convergence pendant l'entraînement.\n","- Temps d'entraînement raisonnable, équilibrant performance et complexité.\n","\n","Comparaison avec les autres modèles :\n","- **U-Net Mini**, utilisé comme baseline, a montré des performances modestes (IoU : 0.626).\n","- Les architectures avancées comme **DeepLabV3+** et **PSPNet** ont offert des résultats solides, mais elles n'ont pas surpassé le FPN avec ResNet50.\n","\n","Choix final :\n","- Le **FPN avec ResNet50** a été retenu comme modèle optimal pour sa capacité à segmenter les scènes complexes tout en préservant les détails critiques.\n","- Il sera intégré dans l'API de prédiction pour fournir des masques de segmentation en temps réel.\n","\n","Perspectives :\n","- Bien que le modèle soit performant, des pistes d'amélioration existent :\n","- Entraîner le modèle avec des images en haute résolution pour plus de précision.\n","- Explorer des backbones plus avancés, comme EfficientNetB7, pour des scènes complexes."],"metadata":{"id":"5q0Js8PhyIRD"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["MgRNYdMq-LpH","eHNr5nxo-OtI","7IqIe-5G-9li","QlagHfGQ_B4T","LoPkAon9Qghc","7GLgVWl_7v22","AksvG5pv-XSF","-RPn4q3I75t9","X2vUP3wI79wq","Jr01aPZPQwmP","mgEl-OZmQ0wj","LXWSd81EQ7n9","xKlMacoXRBCb","Pn_ODNJaRMa_","Rf2nqJWPRSDt","RnwXQyNPRX6U","rqgYfzDwRd4a","gOHkYkQxXRwD","ANNYv9y3YyW4","ahX70CrmZImu","-qyyunQ7CtMN","1by_vFUzDD2c","lPxXABtZZ0AU","ClCSGU7exjk4","gn5jsOy29kKk","HFliVYgW9xfK","PY55f30P94m1","Vun26Ds5-BRb","6UVGlqHG-Ilf","keFF9qx4C89U","roRX92yGEq0l","x5_Ba_-YEzpS","zjvX9Fc9FLIW","pD_3IQSmFWWh","mfzJ7ft8xtFW","skh2MWK7x3_u","mhr5iGTfyA7o","QF7XWWQz2pGo","TqcAmAIo2trI","ZJcmk1siuy9p","Pmx3nJ37u9uO","AfJTmW-Au9DF","OOuQb0lPu6q7","iWm-YK_zu6Dg","fHPmq3KPzpnK","BMgqeTDrz0-G","fUn2pshqz03K","pwTkj4KYz0v6","l3KyaK3vz0nO","rFf-uYyo0Lgu","eVhsoV7J0Lgv","M-mISQbA0Lgv","N5X4VNx80Lgv","GdiCuakb0Lgv","By6j4HdE0Mo_","PDjLlhLI0Mo_","PhviHfR10MpA","ga-OkjeD0MpA","IQ_qU83y0MpA","XU8iupH5yBvL"],"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1N388Aqg28wlURWAnYC41Vr9q3qktofBS","timestamp":1736004001919}],"authorship_tag":"ABX9TyMToko7suQG9jt4Fs8bgKDe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}